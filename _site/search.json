[
  {
    "objectID": "notebooks/index.html",
    "href": "notebooks/index.html",
    "title": "Using VADER to perform simple sentiment analysis",
    "section": "",
    "text": "Using VADER to perform simple sentiment analysis\n\n\nCode\n%store -r reddit_df\nreddit_df = reddit_df\n\nimport pandas as pd\n\n\n\n\nCode\nfrom vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer\nsenti_analyzer = SentimentIntensityAnalyzer()\n\n\n\n\nCode\nreddit_df[\"full_text\"] = reddit_df[\"title\"] + \"\\n\" + reddit_df[\"content\"]\n\n\n\n\nCode\ndef get_sentiment_scores(text):\n    if pd.isna(text) or text == '':\n        return {'neg': 0, 'neu': 0, 'pos': 0, 'compound': 0}\n    return senti_analyzer.polarity_scores(text)\n\nsentiment_scores = reddit_df['full_text'].apply(lambda x: get_sentiment_scores(x))\n\n\n\n\nCode\nreddit_df[\"compound\"] = sentiment_scores.apply(lambda x: x[\"compound\"])\nreddit_df[\"positive\"] = sentiment_scores.apply(lambda x: x[\"pos\"])\nreddit_df[\"negative\"] = sentiment_scores.apply(lambda x: x[\"neg\"])\nreddit_df[\"neutral\"] = sentiment_scores.apply(lambda x: x[\"neu\"])\n\n\n\n\nCode\nreddit_df[\"year\"] = reddit_df[\"date\"].dt.year\n\n\n\n\nCode\ndef assign_covid_period(date):\n    if date &lt; pd.Timestamp(\"2020-03-01\"):\n        return \"Pre-COVID\"\n    elif date &gt;= pd.Timestamp(\"2020-03-01\") and date &lt; pd.Timestamp(\"2023-05-01\"):\n        return \"During COVID\"\n    else:\n        return \"Post-COVID\"\n    \nreddit_df[\"covid_period\"] = reddit_df[\"date\"].apply(assign_covid_period)\n\n\n\n\nCode\nyearly_sentiment = reddit_df.groupby('year').agg({\n    'compound': 'mean',\n    'positive': 'mean',\n    'negative': 'mean',\n    'neutral': 'mean'\n}).round(4)\n\nprint(\"Average Sentiment Scores by Year\")\nprint(yearly_sentiment)\n\n\nAverage Sentiment Scores by Year\n      compound  positive  negative  neutral\nyear                                       \n2020   -0.1997    0.1113    0.1775   0.7112\n2021   -0.1710    0.0979    0.1558   0.7463\n2022   -0.1965    0.1066    0.1556   0.7378\n2023   -0.2497    0.1114    0.1573   0.7313\n2024   -0.2382    0.1056    0.1620   0.7324\n2025   -0.2772    0.1026    0.1616   0.7358\n\n\n\n\nCode\n# Group by COVID period\nperiod_statement = reddit_df.groupby('covid_period').agg({\n    'compound': ['mean', 'std', 'count'],\n    'negative': 'mean',\n    'positive': 'mean'\n}).round(4)\n\nprint(\"Sentiment by COVID Period\")\nprint(period_statement)\n\n\nSentiment by COVID Period\n             compound                negative positive\n                 mean     std  count     mean     mean\ncovid_period                                          \nDuring COVID  -0.1967  0.6350  10517   0.1624   0.1049\nPost-COVID    -0.2485  0.7024   6218   0.1600   0.1073\nPre-COVID     -0.1463  0.6532    576   0.1747   0.1142\n\n\n\n\nCode\nimport plotly.express as px\n\n# Create the base line plot\nfig = px.line(\n    yearly_sentiment,\n    x=yearly_sentiment.index,\n    y=\"compound\",\n    markers=True,  # Creates the 'o' markers\n    text=\"compound\",  # Use the 'compound' column for text labels\n    # Set titles and labels\n    title=\"Reddit Mental Health Sentiment Over Time\",\n    labels={\n        \"index\": \"Year\",  # 'index' because we passed the index as x\n        \"compound\": \"Average Compound Sentiment Score\",\n    },\n)\n\n# Add the vertical 'COVID Start' line\nfig.add_vline(\n    x=2020,\n    line_dash=\"dash\",  # Replaces 'linestyle'\n    line_color=\"red\",  # Replaces 'color'\n    annotation_text=\"COVID Start\",  # This is Plotly's 'label'\n    annotation_position=\"top right\",\n)\n\n# Format the text labels on the points\nfig.update_traces(\n    textposition=\"bottom right\",  # Matches 'ha='center', va='bottom'\n    texttemplate=\"%{text:.3f}\",  # Formats the text like '{score:.3f}'\n)\n\n# Adjust layout size and grid (Plotly's grid is on by default)\nfig.update_layout(\n    width=1000,  # Roughly matches figsize=(10, 6)\n    height=600,\n    xaxis_gridcolor=\"rgba(0,0,0,0.1)\",  # Lighter grid, like 'alpha=0.3'\n    yaxis_gridcolor=\"rgba(0,0,0,0.1)\",\n)\n\nfig.show()\n\n\nUnable to display output for mime type(s): application/vnd.plotly.v1+json\n\n\n\n\nCode\n# Check data distribution\nprint(\"Posts per year:\")\nprint(reddit_df['year'].value_counts().sort_index())\n\n# Compare pre vs during COVID (if you have that data)\nprint(\"\\nAverage compound sentiment by period:\")\nfor period in ['Pre-COVID', 'During COVID', 'Post-COVID']:\n    data = reddit_df[reddit_df['covid_period'] == period]\n    if len(data) &gt; 0:\n        print(f\"{period}: {data['compound'].mean():.4f} (n={len(data)})\")\n\n\nPosts per year:\nyear\n2020    3536\n2021    3567\n2022    3159\n2023    2775\n2024    2993\n2025    1281\nName: count, dtype: int64\n\nAverage compound sentiment by period:\nPre-COVID: -0.1463 (n=576)\nDuring COVID: -0.1967 (n=10517)\nPost-COVID: -0.2485 (n=6218)\n\n\n\n\nCode\nreddit_sent_df = reddit_df\n\n\n\n\nCode\n%store reddit_sent_df\n\n\nStored 'reddit_sent_df' (DataFrame)"
  },
  {
    "objectID": "notebooks/data_word_frequency.html",
    "href": "notebooks/data_word_frequency.html",
    "title": "Identifying Shifts in Mental Health Discourse across COVID periods",
    "section": "",
    "text": "Code\nimport re\nfrom collections import Counter\nfrom sklearn.feature_extraction.text import TfidfTransformer\nimport numpy as np\nimport pandas as pd\nimport nltk\nfrom nltk.corpus import stopwords\n\nnltk.download(\"stopwords\")\n\n\n[nltk_data] Downloading package stopwords to\n[nltk_data]     /home/beherya/nltk_data...\n[nltk_data]   Package stopwords is already up-to-date!\n\n\nTrue\n\n\nText Preprocessing Function\n\n\nCode\ndef preprocess_text(text):\n    if pd.isna(text):\n        return []\n\n    text = text.lower()\n\n    text = re.sub(r\"http\\S+|www.\\S+\", \"\", text)\n    text = re.sub(r\"[^a-zA-Z\\s]\", \"\", text)\n\n    stopwords_set = set(stopwords.words(\"english\"))\n\n    custom_stop_words = [\n        \"like\",\n        \"dont\",\n        \"get\",\n        \"ive\",\n        \"want\",\n        \"even\",\n        \"really\",\n        \"things\",\n        \"something\",\n        \"get\",\n        \"thing\",\n        \"year\",\n        \"years\",\n        \"much\",\n        \"cant\",\n        \"would\",\n        \"much\",\n        \"back\",\n        \"also\"\n    ]\n\n    stopwords_set.update(custom_stop_words)\n\n    words = text.split()\n    words = [word for word in words if len(word) &gt; 2 and word not in stopwords_set]\n\n    return words\n\n\nprint(\"Done\")\n\n\nDone\n\n\nThis function cleans text by converting to lowercase, removing URLs and punctuation, and filtering out stopwords (both standard English and custom ones). It only keeps words longer than 2 characters. The custom stopwords are conversational filler words that don’t add analytical value.\n\n\nCode\n# Get word frequency by period\n\ndef get_words_by_period(df, text_column=\"full_text\"):\n    period_word_counts = {}\n    \n    for period in df[\"covid_period\"].unique():\n        period_data = df[df[\"covid_period\"] == period]\n        \n        all_words = []\n        for text in period_data[text_column]:\n            all_words.extend(preprocess_text(text))\n            \n        word_counts = Counter(all_words)\n        period_word_counts[period] = word_counts\n        \n    return period_word_counts\n\n\nThis function iterates through each COVID period, preprocesses all text from that period, and uses Counter to tally word frequencies. It returns a dictionary where keys are period names and values are word count dictionaries.\n\n\nCode\n%store -r reddit_sent_df\n\n\n\n\nCode\n\nperiod_word_counts = get_words_by_period(reddit_sent_df)\n\n\n\n\nCode\ndef get_top_words(word_counts, n=20):\n    return word_counts.most_common(n)\n\n\n\n\nCode\nprint(\"Top 10 words by COVID Period\")\nfor period in [\"Pre-COVID\", \"During COVID\", \"Post-COVID\"]:\n    if period in period_word_counts:\n        print(f\"\\n{period}:\")\n        top_words = get_top_words(period_word_counts[period], 10)\n        for word, count in top_words:\n            print(f\"\\t{word}:\\t{count}\")\n\n\nTop 10 words by COVID Period\n\nPre-COVID:\n    feel:   348\n    know:   250\n    anxiety:    243\n    time:   206\n    life:   189\n    people: 180\n    new:    161\n    help:   155\n    going:  146\n    friends:    130\n\nDuring COVID:\n    feel:   7192\n    know:   5042\n    anxiety:    4401\n    life:   3607\n    time:   3601\n    people: 3105\n    one:    2737\n    think:  2669\n    help:   2641\n    going:  2479\n\nPost-COVID:\n    feel:   5665\n    know:   3855\n    anxiety:    3813\n    life:   3028\n    time:   2760\n    people: 2221\n    think:  2064\n    going:  2039\n    one:    1999\n    help:   1886\n\n\nInterpretation: The raw frequencies tell a striking story:\nScaling Effect: The word “feel” appears 348 times pre-COVID but jumps to 7,192 during COVID—a 20.7x increase. This reflects both the volume increase (18x more posts) and a slight intensification of emotional expression.\nConsistency Across Periods: The top 10 words are remarkably similar across all three periods. “Feel,” “know,” “anxiety,” “life,” “time,” “people,” and “help” dominate all three lists, suggesting these are core themes in mental health discussions regardless of external circumstances.\nProportional Analysis: Pre-COVID: “anxiety” appears 243 times (0.70 times per post on average)\nDuring COVID: “anxiety” appears 4,401 times (0.42 times per post)\nPost-COVID: “anxiety” appears 3,813 times (0.61 times per post)\nInterestingly, “anxiety” was mentioned proportionally less per post during COVID, possibly because discussions became more varied (including topics like isolation, work, school) rather than purely anxiety-focused.\nNew Words: “One” and “think” enter the top 10 during and after COVID, suggesting more reflective, cognitive language emerged during the pandemic.\n\n\nCode\nfrom wordcloud import WordCloud\nimport matplotlib.pyplot as plt\n\n\n\n\nCode\nwordcloud_pre_covid = WordCloud(\n    width=800,\n    height=400,\n    background_color=\"white\",\n    colormap=\"viridis\",\n    max_words=100\n).generate_from_frequencies(period_word_counts[\"Pre-COVID\"])\n\nwordcloud_during_covid = WordCloud(\n    width=800,\n    height=400,\n    background_color=\"white\",\n    colormap=\"viridis\",\n    max_words=100\n).generate_from_frequencies(period_word_counts[\"During COVID\"])\n\nwordcloud_post_covid = WordCloud(\n    width=800,\n    height=400,\n    background_color=\"white\",\n    colormap=\"viridis\",\n    max_words=100\n).generate_from_frequencies(period_word_counts[\"Post-COVID\"])\n\n\nThis creates visual word clouds where word size corresponds to frequency. The viridis colormap provides a gradient from purple to yellow, and max_words=100 limits the display to the 100 most frequent words.\n\n\nCode\nplt.figure(figsize=(10, 5))\nplt.imshow(wordcloud_pre_covid, interpolation=\"bilinear\")\nplt.title(\"World Cloud, Pre-COVID\")\nplt.axis(\"off\")\nplt.show()",
    "crumbs": [
      "View on GitHub",
      "Word Frequency"
    ]
  },
  {
    "objectID": "notebooks/data_word_frequency.html#identifying-shifts-in-mental-health-discourse-across-covid-periods",
    "href": "notebooks/data_word_frequency.html#identifying-shifts-in-mental-health-discourse-across-covid-periods",
    "title": "Identifying Shifts in Mental Health Discourse across COVID periods",
    "section": "",
    "text": "Code\nimport re\nfrom collections import Counter\nfrom sklearn.feature_extraction.text import TfidfTransformer\nimport numpy as np\nimport pandas as pd\nimport nltk\nfrom nltk.corpus import stopwords\n\nnltk.download(\"stopwords\")\n\n\n[nltk_data] Downloading package stopwords to\n[nltk_data]     /home/beherya/nltk_data...\n[nltk_data]   Package stopwords is already up-to-date!\n\n\nTrue\n\n\nText Preprocessing Function\n\n\nCode\ndef preprocess_text(text):\n    if pd.isna(text):\n        return []\n\n    text = text.lower()\n\n    text = re.sub(r\"http\\S+|www.\\S+\", \"\", text)\n    text = re.sub(r\"[^a-zA-Z\\s]\", \"\", text)\n\n    stopwords_set = set(stopwords.words(\"english\"))\n\n    custom_stop_words = [\n        \"like\",\n        \"dont\",\n        \"get\",\n        \"ive\",\n        \"want\",\n        \"even\",\n        \"really\",\n        \"things\",\n        \"something\",\n        \"get\",\n        \"thing\",\n        \"year\",\n        \"years\",\n        \"much\",\n        \"cant\",\n        \"would\",\n        \"much\",\n        \"back\",\n        \"also\"\n    ]\n\n    stopwords_set.update(custom_stop_words)\n\n    words = text.split()\n    words = [word for word in words if len(word) &gt; 2 and word not in stopwords_set]\n\n    return words\n\n\nprint(\"Done\")\n\n\nDone\n\n\nThis function cleans text by converting to lowercase, removing URLs and punctuation, and filtering out stopwords (both standard English and custom ones). It only keeps words longer than 2 characters. The custom stopwords are conversational filler words that don’t add analytical value.\n\n\nCode\n# Get word frequency by period\n\ndef get_words_by_period(df, text_column=\"full_text\"):\n    period_word_counts = {}\n    \n    for period in df[\"covid_period\"].unique():\n        period_data = df[df[\"covid_period\"] == period]\n        \n        all_words = []\n        for text in period_data[text_column]:\n            all_words.extend(preprocess_text(text))\n            \n        word_counts = Counter(all_words)\n        period_word_counts[period] = word_counts\n        \n    return period_word_counts\n\n\nThis function iterates through each COVID period, preprocesses all text from that period, and uses Counter to tally word frequencies. It returns a dictionary where keys are period names and values are word count dictionaries.\n\n\nCode\n%store -r reddit_sent_df\n\n\n\n\nCode\n\nperiod_word_counts = get_words_by_period(reddit_sent_df)\n\n\n\n\nCode\ndef get_top_words(word_counts, n=20):\n    return word_counts.most_common(n)\n\n\n\n\nCode\nprint(\"Top 10 words by COVID Period\")\nfor period in [\"Pre-COVID\", \"During COVID\", \"Post-COVID\"]:\n    if period in period_word_counts:\n        print(f\"\\n{period}:\")\n        top_words = get_top_words(period_word_counts[period], 10)\n        for word, count in top_words:\n            print(f\"\\t{word}:\\t{count}\")\n\n\nTop 10 words by COVID Period\n\nPre-COVID:\n    feel:   348\n    know:   250\n    anxiety:    243\n    time:   206\n    life:   189\n    people: 180\n    new:    161\n    help:   155\n    going:  146\n    friends:    130\n\nDuring COVID:\n    feel:   7192\n    know:   5042\n    anxiety:    4401\n    life:   3607\n    time:   3601\n    people: 3105\n    one:    2737\n    think:  2669\n    help:   2641\n    going:  2479\n\nPost-COVID:\n    feel:   5665\n    know:   3855\n    anxiety:    3813\n    life:   3028\n    time:   2760\n    people: 2221\n    think:  2064\n    going:  2039\n    one:    1999\n    help:   1886\n\n\nInterpretation: The raw frequencies tell a striking story:\nScaling Effect: The word “feel” appears 348 times pre-COVID but jumps to 7,192 during COVID—a 20.7x increase. This reflects both the volume increase (18x more posts) and a slight intensification of emotional expression.\nConsistency Across Periods: The top 10 words are remarkably similar across all three periods. “Feel,” “know,” “anxiety,” “life,” “time,” “people,” and “help” dominate all three lists, suggesting these are core themes in mental health discussions regardless of external circumstances.\nProportional Analysis: Pre-COVID: “anxiety” appears 243 times (0.70 times per post on average)\nDuring COVID: “anxiety” appears 4,401 times (0.42 times per post)\nPost-COVID: “anxiety” appears 3,813 times (0.61 times per post)\nInterestingly, “anxiety” was mentioned proportionally less per post during COVID, possibly because discussions became more varied (including topics like isolation, work, school) rather than purely anxiety-focused.\nNew Words: “One” and “think” enter the top 10 during and after COVID, suggesting more reflective, cognitive language emerged during the pandemic.\n\n\nCode\nfrom wordcloud import WordCloud\nimport matplotlib.pyplot as plt\n\n\n\n\nCode\nwordcloud_pre_covid = WordCloud(\n    width=800,\n    height=400,\n    background_color=\"white\",\n    colormap=\"viridis\",\n    max_words=100\n).generate_from_frequencies(period_word_counts[\"Pre-COVID\"])\n\nwordcloud_during_covid = WordCloud(\n    width=800,\n    height=400,\n    background_color=\"white\",\n    colormap=\"viridis\",\n    max_words=100\n).generate_from_frequencies(period_word_counts[\"During COVID\"])\n\nwordcloud_post_covid = WordCloud(\n    width=800,\n    height=400,\n    background_color=\"white\",\n    colormap=\"viridis\",\n    max_words=100\n).generate_from_frequencies(period_word_counts[\"Post-COVID\"])\n\n\nThis creates visual word clouds where word size corresponds to frequency. The viridis colormap provides a gradient from purple to yellow, and max_words=100 limits the display to the 100 most frequent words.\n\n\nCode\nplt.figure(figsize=(10, 5))\nplt.imshow(wordcloud_pre_covid, interpolation=\"bilinear\")\nplt.title(\"World Cloud, Pre-COVID\")\nplt.axis(\"off\")\nplt.show()",
    "crumbs": [
      "View on GitHub",
      "Word Frequency"
    ]
  },
  {
    "objectID": "notebooks/data_cleaning.html",
    "href": "notebooks/data_cleaning.html",
    "title": "Attempting to Make Sense of the Reddit Data",
    "section": "",
    "text": "Attempting to Make Sense of the Reddit Data\nReading the data which we wrote to reddit_data.csv\n\n\nCode\nimport pandas as pd\n\nreddit_df = pd.read_csv(\"../reddit_data.csv\")\n\n\n\n\nCode\nreddit_df.head()\n\n\n\n\n\n\n\n\n\n\ntitle\ncontent\ndate\nsubreddit\nlink\n\n\n\n\n0\nHows everyone doing on this new years eve?\nI know as well as everyone new years makes you...\n1.577837e+09\ndepression\nhttps://www.reddit.com/r/depression/comments/e...\n\n\n1\nWaiting for my mind to have a breakdown once t...\n[deleted]\n1.577837e+09\ndepression\nhttps://www.reddit.com/r/depression/comments/e...\n\n\n2\nWorst decade ever\nGood bye, 2010s decade, and fuck you.\\n\\nHopef...\n1.577837e+09\ndepression\nhttps://www.reddit.com/r/depression/comments/e...\n\n\n3\nMy new years resolution\nI'm gonna get my ass into a therapists office,...\n1.577837e+09\ndepression\nhttps://www.reddit.com/r/depression/comments/e...\n\n\n4\nNew year\nSomone else Feeling like 2020 will be there la...\n1.577837e+09\ndepression\nhttps://www.reddit.com/r/depression/comments/e...\n\n\n\n\n\n\n\n\n\n\nCode\nreddit_df[\"date\"] = pd.to_datetime(reddit_df[\"date\"], unit=\"s\")\n\n\n\n\nCode\nreddit_df[\"date\"]\n\n\n0       2020-01-01 00:00:56\n1       2020-01-01 00:01:25\n2       2020-01-01 00:02:44\n3       2020-01-01 00:03:57\n4       2020-01-01 00:04:02\n                ...        \n19495   2025-05-01 11:44:22\n19496   2025-05-01 12:02:04\n19497   2025-05-01 12:02:04\n19498   2025-05-01 12:31:34\n19499   2025-05-01 12:38:23\nName: date, Length: 19500, dtype: datetime64[ns]\n\n\n\n\nCode\nlen(reddit_df)\n\n\n19500\n\n\n\n\nCode\nreddit_df = reddit_df.dropna()\n\n\n\n\nCode\nlen(reddit_df)\n\n\n17311\n\n\n\nRemoving all the [removed] posts in the data frame\n\n\nCode\nreddit_df[\"content\"].replace(\"[removed]\", \"\", inplace=True)\n\n\n/tmp/ipykernel_58959/2758632231.py:1: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\nThe behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n\nFor example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n\n\n  reddit_df[\"content\"].replace(\"[removed]\", \"\", inplace=True)\n\n\n\n\nRemoving all the [deleted] posts in the data frame\n\n\nCode\nreddit_df[\"content\"].replace(\"[deleted]\", \"\", inplace=True)\n\n\n/tmp/ipykernel_58959/4121564709.py:1: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\nThe behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n\nFor example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n\n\n  reddit_df[\"content\"].replace(\"[deleted]\", \"\", inplace=True)\n\n\n\n\nCode\nreddit_df\n\n\n\n\n\n\n\n\n\n\ntitle\ncontent\ndate\nsubreddit\nlink\n\n\n\n\n0\nHows everyone doing on this new years eve?\nI know as well as everyone new years makes you...\n2020-01-01 00:00:56\ndepression\nhttps://www.reddit.com/r/depression/comments/e...\n\n\n1\nWaiting for my mind to have a breakdown once t...\n\n2020-01-01 00:01:25\ndepression\nhttps://www.reddit.com/r/depression/comments/e...\n\n\n2\nWorst decade ever\nGood bye, 2010s decade, and fuck you.\\n\\nHopef...\n2020-01-01 00:02:44\ndepression\nhttps://www.reddit.com/r/depression/comments/e...\n\n\n3\nMy new years resolution\nI'm gonna get my ass into a therapists office,...\n2020-01-01 00:03:57\ndepression\nhttps://www.reddit.com/r/depression/comments/e...\n\n\n4\nNew year\nSomone else Feeling like 2020 will be there la...\n2020-01-01 00:04:02\ndepression\nhttps://www.reddit.com/r/depression/comments/e...\n\n\n...\n...\n...\n...\n...\n...\n\n\n19493\nFight on or tactical retreat?\nHi everyone. I've got diagnosed anxiety disord...\n2025-05-01 10:39:03\nAnxiety\nhttps://www.reddit.com/r/Anxiety/comments/1kc5...\n\n\n19494\nThe future is just so terrifying. I'm 18 but I...\nI've just been stressed non stop about everyth...\n2025-05-01 10:59:47\nAnxiety\nhttps://www.reddit.com/r/Anxiety/comments/1kc5...\n\n\n19497\nWorried about going to my first wedding\nI guess it’s not my first wedding but my only ...\n2025-05-01 12:02:04\nAnxiety\nhttps://www.reddit.com/r/Anxiety/comments/1kc6...\n\n\n19498\nCovid Anxiety\nCovid anxiety\\n\\nCOVID anxiety has destroyed m...\n2025-05-01 12:31:34\nAnxiety\nhttps://www.reddit.com/r/Anxiety/comments/1kc7...\n\n\n19499\nWhat type of therapy helped with your anxiety?\nI'm a 33-year-old male and have been experienc...\n2025-05-01 12:38:23\nAnxiety\nhttps://www.reddit.com/r/Anxiety/comments/1kc7...\n\n\n\n\n17311 rows × 5 columns\n\n\n\n\n\n\nCode\n%store reddit_df\n\n\nStored 'reddit_df' (DataFrame)\n\n\n\n\n\nTesting how VADER works, for sentiment analysis\n\n\nCode\nfrom vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer\n\n\n\n\nCode\nsenti_analyzer = SentimentIntensityAnalyzer()\nsent_results = senti_analyzer.polarity_scores(\"Too depressed to be productive, and not depressed enough to kill myself\")\n\n\n\n\nCode\nsent_results\n\n\n{'neg': 0.406, 'neu': 0.457, 'pos': 0.137, 'compound': -0.7429}\n\n\nhttps://hex.tech/templates/sentiment-analysis/vader-sentiment-analysis/",
    "crumbs": [
      "View on GitHub",
      "Data Cleaning"
    ]
  },
  {
    "objectID": "notebooks/comparing_subreddits.html",
    "href": "notebooks/comparing_subreddits.html",
    "title": "",
    "section": "",
    "text": "CodeShow All CodeHide All Code\nComparing Subreddits\nCode\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport re\nimport warnings\nCode\n%store -r reddit_sent_df\nCode\nsentiment_by_sub = reddit_sent_df.groupby(['covid_period', 'subreddit'])['compound'].mean().reset_index()\nCode\nsentiment_by_sub['covid_period'] = pd.Categorical(sentiment_by_sub['covid_period'], \n                                                  categories=['Pre-COVID', 'During COVID', 'Post-COVID'], \n                                                  ordered=True)\nCode\nsentiment_by_sub = sentiment_by_sub.sort_values('covid_period')\nThis groups the data by COVID period and subreddit, then calculates the mean compound sentiment score for each combination. The compound score is from VADER sentiment analysis (ranges from -1 most negative to +1 most positive). The code then converts the period to an ordered categorical variable to ensure proper chronological ordering in visualizations.\nCode\nimport plotly.express as px\nimport plotly.io as pio\npio.renderers.default = \"notebook\"\n\nfig = px.bar(\n    sentiment_by_sub,\n    x='covid_period',\n    y='compound',\n    color='subreddit',\n    barmode='group',\n    title='Average Sentiment Score by Subreddit and COVID Period'\n)\nfig.show()\nCreates an interactive grouped bar chart using Plotly Express. Each subreddit is represented by a different color, and bars are grouped by COVID period on the x-axis.",
    "crumbs": [
      "View on GitHub",
      "Comparing Subreddits"
    ]
  },
  {
    "objectID": "notebooks/collecting_over_5_years.html",
    "href": "notebooks/collecting_over_5_years.html",
    "title": "Collecting Data over 5 years, start of COVID to now",
    "section": "",
    "text": "Collecting Data over 5 years, start of COVID to now\n\n\nCode\nimport requests\nimport datetime\nimport time\nimport json\n\n\nThis code defines a function to fetch a list of Reddit posts from the Pushshift API. It searches a specific subreddit for posts made between a start_time and end_time, returning up to 100 of the oldest posts first. The function handles errors or empty results by returning an empty list.\n\n\nCode\ndef get_pushshift_monthly_chunk(subreddit, start_time, end_time, limit=100):\n    \"\"\"Fetch a single chunk of data from Pushshift API\"\"\"\n    base_url = \"https://api.pullpush.io/reddit/search/submission\"\n\n    params = {\n        \"subreddit\": subreddit,\n        \"after\": int(start_time.timestamp()),\n        \"before\": int(end_time.timestamp()),\n        \"size\": limit,  # Fetch up to 'limit' posts (max 100)\n        \"sort\": \"asc\",\n        \"sort_type\": \"created_utc\",\n    }\n\n    try:\n        response = requests.get(base_url, params=params)\n        data = response.json()\n\n        if \"data\" not in data or len(data[\"data\"]) == 0:\n            return []  # Return an empty list if no posts\n\n        return data[\"data\"]  # Return the list of posts found\n\n    except Exception as e:\n        print(f\"Error fetching chunk: {e}\")\n        return []  # Return an empty list on error\n\n\nAlright, so to start my main data collection, I first set up these variables. I create an empty list called all_posts which is where I’ll store all the data as I gather it. I then define the total time window I want to analyze, setting the start_date to January 1, 2020, and the end_date to the current time.\n\n\nCode\nall_posts = []\nstart_date = datetime.datetime(2020, 1, 1, tzinfo=datetime.timezone.utc)\nend_date = datetime.datetime.now(datetime.timezone.utc)\ncurrent_start = start_date\n\n\nThis is the main data collection loop from my collecting_over_5_years.ipynb notebook, which iterates month by month from January 2020 until the present. For each month, I loop through my three target subreddits (r/depression, r/mentalhealth, and r/Anxiety) and call the get_pushshift_monthly_chunk function to fetch 100 posts from each. I add these new posts to my main all_posts list and include a 3-second time.sleep call to avoid overwhelming the API.\n\n\nCode\nprint(\n    f\"Collecting up to 100 posts per month from {start_date.date()} to {end_date.date()}...\"\n)\n\nsubreddits_to_fetch = [\"depression\", \"mentalhealth\", \"Anxiety\"]\n\nwhile current_start &lt; end_date:\n    # 1. Calculate the end of the current month\n    if current_start.month == 12:\n        next_month_start = current_start.replace(\n            year=current_start.year + 1, month=1, day=1\n        )\n    else:\n        next_month_start = current_start.replace(month=current_start.month + 1, day=1)\n\n    current_end = next_month_start - datetime.timedelta(seconds=1)\n\n    # 2. Ensure our end date doesn't go into the future\n    if current_end &gt; end_date:\n        current_end = end_date\n\n    # 3. Fetch the chunk for this month\n    print(f\"--- Fetching for {current_start.date()} to {current_end.date()} ---\")\n    \n    for subreddit_name in subreddits_to_fetch:\n        print(f\"Fetching r/{subreddit_name}...\")\n\n        # Call our modified function for this month's window\n        posts_chunk = get_pushshift_monthly_chunk(\n            subreddit_name, current_start, current_end, limit=100\n        )\n\n        all_posts.extend(posts_chunk)\n\n        print(f\"...found {len(posts_chunk)} posts. Total collected: {len(all_posts)}\")\n        time.sleep(3)\n        \n    current_start = next_month_start\n\nprint(\"\\n--- FINISHED ALL SEARCHES ---\")\nprint(f\"Successfully collected a total of {len(all_posts)} posts.\")\n\n\nCollecting up to 100 posts per month from 2020-01-01 to 2025-11-04...\n--- Fetching for 2020-01-01 to 2020-01-31 ---\nFetching r/depression...\n...found 100 posts. Total collected: 100\nFetching r/mentalhealth...\n...found 100 posts. Total collected: 200\nFetching r/Anxiety...\n...found 100 posts. Total collected: 300\n--- Fetching for 2020-02-01 to 2020-02-29 ---\nFetching r/depression...\n...found 100 posts. Total collected: 400\nFetching r/mentalhealth...\n...found 100 posts. Total collected: 500\nFetching r/Anxiety...\n...found 100 posts. Total collected: 600\n--- Fetching for 2020-03-01 to 2020-03-31 ---\nFetching r/depression...\n...found 100 posts. Total collected: 700\nFetching r/mentalhealth...\n...found 100 posts. Total collected: 800\nFetching r/Anxiety...\n...found 100 posts. Total collected: 900\n--- Fetching for 2020-04-01 to 2020-04-30 ---\nFetching r/depression...\n...found 100 posts. Total collected: 1000\nFetching r/mentalhealth...\n...found 100 posts. Total collected: 1100\nFetching r/Anxiety...\n...found 100 posts. Total collected: 1200\n--- Fetching for 2020-05-01 to 2020-05-31 ---\nFetching r/depression...\n...found 100 posts. Total collected: 1300\nFetching r/mentalhealth...\n...found 100 posts. Total collected: 1400\nFetching r/Anxiety...\n...found 100 posts. Total collected: 1500\n--- Fetching for 2020-06-01 to 2020-06-30 ---\nFetching r/depression...\n...found 100 posts. Total collected: 1600\nFetching r/mentalhealth...\n...found 100 posts. Total collected: 1700\nFetching r/Anxiety...\n...found 100 posts. Total collected: 1800\n--- Fetching for 2020-07-01 to 2020-07-31 ---\nFetching r/depression...\n...found 100 posts. Total collected: 1900\nFetching r/mentalhealth...\n...found 100 posts. Total collected: 2000\nFetching r/Anxiety...\n...found 100 posts. Total collected: 2100\n--- Fetching for 2020-08-01 to 2020-08-31 ---\nFetching r/depression...\n...found 100 posts. Total collected: 2200\nFetching r/mentalhealth...\n...found 100 posts. Total collected: 2300\nFetching r/Anxiety...\n...found 100 posts. Total collected: 2400\n--- Fetching for 2020-09-01 to 2020-09-30 ---\nFetching r/depression...\n...found 100 posts. Total collected: 2500\nFetching r/mentalhealth...\n...found 100 posts. Total collected: 2600\nFetching r/Anxiety...\n...found 100 posts. Total collected: 2700\n--- Fetching for 2020-10-01 to 2020-10-31 ---\nFetching r/depression...\n...found 100 posts. Total collected: 2800\nFetching r/mentalhealth...\n...found 100 posts. Total collected: 2900\nFetching r/Anxiety...\n...found 100 posts. Total collected: 3000\n--- Fetching for 2020-11-01 to 2020-11-30 ---\nFetching r/depression...\n...found 100 posts. Total collected: 3100\nFetching r/mentalhealth...\n...found 100 posts. Total collected: 3200\nFetching r/Anxiety...\n...found 100 posts. Total collected: 3300\n--- Fetching for 2020-12-01 to 2020-12-31 ---\nFetching r/depression...\n...found 100 posts. Total collected: 3400\nFetching r/mentalhealth...\n...found 100 posts. Total collected: 3500\nFetching r/Anxiety...\n...found 100 posts. Total collected: 3600\n--- Fetching for 2021-01-01 to 2021-01-31 ---\nFetching r/depression...\n...found 100 posts. Total collected: 3700\nFetching r/mentalhealth...\n...found 100 posts. Total collected: 3800\nFetching r/Anxiety...\n...found 100 posts. Total collected: 3900\n--- Fetching for 2021-02-01 to 2021-02-28 ---\nFetching r/depression...\n...found 100 posts. Total collected: 4000\nFetching r/mentalhealth...\n...found 100 posts. Total collected: 4100\nFetching r/Anxiety...\n...found 100 posts. Total collected: 4200\n--- Fetching for 2021-03-01 to 2021-03-31 ---\nFetching r/depression...\n...found 100 posts. Total collected: 4300\nFetching r/mentalhealth...\n...found 100 posts. Total collected: 4400\nFetching r/Anxiety...\n...found 100 posts. Total collected: 4500\n--- Fetching for 2021-04-01 to 2021-04-30 ---\nFetching r/depression...\n...found 100 posts. Total collected: 4600\nFetching r/mentalhealth...\n...found 100 posts. Total collected: 4700\nFetching r/Anxiety...\n...found 100 posts. Total collected: 4800\n--- Fetching for 2021-05-01 to 2021-05-31 ---\nFetching r/depression...\n...found 100 posts. Total collected: 4900\nFetching r/mentalhealth...\n...found 100 posts. Total collected: 5000\nFetching r/Anxiety...\n...found 100 posts. Total collected: 5100\n--- Fetching for 2021-06-01 to 2021-06-30 ---\nFetching r/depression...\n...found 100 posts. Total collected: 5200\nFetching r/mentalhealth...\n...found 100 posts. Total collected: 5300\nFetching r/Anxiety...\n...found 100 posts. Total collected: 5400\n--- Fetching for 2021-07-01 to 2021-07-31 ---\nFetching r/depression...\n...found 100 posts. Total collected: 5500\nFetching r/mentalhealth...\n...found 100 posts. Total collected: 5600\nFetching r/Anxiety...\n...found 100 posts. Total collected: 5700\n--- Fetching for 2021-08-01 to 2021-08-31 ---\nFetching r/depression...\n...found 100 posts. Total collected: 5800\nFetching r/mentalhealth...\n...found 100 posts. Total collected: 5900\nFetching r/Anxiety...\n...found 100 posts. Total collected: 6000\n--- Fetching for 2021-09-01 to 2021-09-30 ---\nFetching r/depression...\n...found 100 posts. Total collected: 6100\nFetching r/mentalhealth...\n...found 100 posts. Total collected: 6200\nFetching r/Anxiety...\n...found 100 posts. Total collected: 6300\n--- Fetching for 2021-10-01 to 2021-10-31 ---\nFetching r/depression...\n...found 100 posts. Total collected: 6400\nFetching r/mentalhealth...\n...found 100 posts. Total collected: 6500\nFetching r/Anxiety...\n...found 100 posts. Total collected: 6600\n--- Fetching for 2021-11-01 to 2021-11-30 ---\nFetching r/depression...\n...found 100 posts. Total collected: 6700\nFetching r/mentalhealth...\n...found 100 posts. Total collected: 6800\nFetching r/Anxiety...\n...found 100 posts. Total collected: 6900\n--- Fetching for 2021-12-01 to 2021-12-31 ---\nFetching r/depression...\n...found 100 posts. Total collected: 7000\nFetching r/mentalhealth...\n...found 100 posts. Total collected: 7100\nFetching r/Anxiety...\n...found 100 posts. Total collected: 7200\n--- Fetching for 2022-01-01 to 2022-01-31 ---\nFetching r/depression...\n...found 100 posts. Total collected: 7300\nFetching r/mentalhealth...\n...found 100 posts. Total collected: 7400\nFetching r/Anxiety...\n...found 100 posts. Total collected: 7500\n--- Fetching for 2022-02-01 to 2022-02-28 ---\nFetching r/depression...\n...found 100 posts. Total collected: 7600\nFetching r/mentalhealth...\n...found 100 posts. Total collected: 7700\nFetching r/Anxiety...\n...found 100 posts. Total collected: 7800\n--- Fetching for 2022-03-01 to 2022-03-31 ---\nFetching r/depression...\n...found 100 posts. Total collected: 7900\nFetching r/mentalhealth...\n...found 100 posts. Total collected: 8000\nFetching r/Anxiety...\n...found 100 posts. Total collected: 8100\n--- Fetching for 2022-04-01 to 2022-04-30 ---\nFetching r/depression...\n...found 100 posts. Total collected: 8200\nFetching r/mentalhealth...\n...found 100 posts. Total collected: 8300\nFetching r/Anxiety...\n...found 100 posts. Total collected: 8400\n--- Fetching for 2022-05-01 to 2022-05-31 ---\nFetching r/depression...\n...found 100 posts. Total collected: 8500\nFetching r/mentalhealth...\n...found 100 posts. Total collected: 8600\nFetching r/Anxiety...\n...found 100 posts. Total collected: 8700\n--- Fetching for 2022-06-01 to 2022-06-30 ---\nFetching r/depression...\n...found 100 posts. Total collected: 8800\nFetching r/mentalhealth...\n...found 100 posts. Total collected: 8900\nFetching r/Anxiety...\n...found 100 posts. Total collected: 9000\n--- Fetching for 2022-07-01 to 2022-07-31 ---\nFetching r/depression...\n...found 100 posts. Total collected: 9100\nFetching r/mentalhealth...\n...found 100 posts. Total collected: 9200\nFetching r/Anxiety...\n...found 100 posts. Total collected: 9300\n--- Fetching for 2022-08-01 to 2022-08-31 ---\nFetching r/depression...\n...found 100 posts. Total collected: 9400\nFetching r/mentalhealth...\n...found 100 posts. Total collected: 9500\nFetching r/Anxiety...\n...found 100 posts. Total collected: 9600\n--- Fetching for 2022-09-01 to 2022-09-30 ---\nFetching r/depression...\n...found 100 posts. Total collected: 9700\nFetching r/mentalhealth...\n...found 100 posts. Total collected: 9800\nFetching r/Anxiety...\n...found 100 posts. Total collected: 9900\n--- Fetching for 2022-10-01 to 2022-10-31 ---\nFetching r/depression...\n...found 100 posts. Total collected: 10000\nFetching r/mentalhealth...\n...found 100 posts. Total collected: 10100\nFetching r/Anxiety...\n...found 100 posts. Total collected: 10200\n--- Fetching for 2022-11-01 to 2022-11-30 ---\nFetching r/depression...\n...found 100 posts. Total collected: 10300\nFetching r/mentalhealth...\n...found 100 posts. Total collected: 10400\nFetching r/Anxiety...\n...found 100 posts. Total collected: 10500\n--- Fetching for 2022-12-01 to 2022-12-31 ---\nFetching r/depression...\n...found 100 posts. Total collected: 10600\nFetching r/mentalhealth...\n...found 100 posts. Total collected: 10700\nFetching r/Anxiety...\n...found 100 posts. Total collected: 10800\n--- Fetching for 2023-01-01 to 2023-01-31 ---\nFetching r/depression...\n...found 100 posts. Total collected: 10900\nFetching r/mentalhealth...\n...found 100 posts. Total collected: 11000\nFetching r/Anxiety...\n...found 100 posts. Total collected: 11100\n--- Fetching for 2023-02-01 to 2023-02-28 ---\nFetching r/depression...\n...found 100 posts. Total collected: 11200\nFetching r/mentalhealth...\n...found 100 posts. Total collected: 11300\nFetching r/Anxiety...\n...found 100 posts. Total collected: 11400\n--- Fetching for 2023-03-01 to 2023-03-31 ---\nFetching r/depression...\n...found 100 posts. Total collected: 11500\nFetching r/mentalhealth...\n...found 100 posts. Total collected: 11600\nFetching r/Anxiety...\n...found 100 posts. Total collected: 11700\n--- Fetching for 2023-04-01 to 2023-04-30 ---\nFetching r/depression...\n...found 100 posts. Total collected: 11800\nFetching r/mentalhealth...\n...found 100 posts. Total collected: 11900\nFetching r/Anxiety...\n...found 100 posts. Total collected: 12000\n--- Fetching for 2023-05-01 to 2023-05-31 ---\nFetching r/depression...\n...found 100 posts. Total collected: 12100\nFetching r/mentalhealth...\n...found 100 posts. Total collected: 12200\nFetching r/Anxiety...\n...found 100 posts. Total collected: 12300\n--- Fetching for 2023-06-01 to 2023-06-30 ---\nFetching r/depression...\n...found 100 posts. Total collected: 12400\nFetching r/mentalhealth...\n...found 100 posts. Total collected: 12500\nFetching r/Anxiety...\n...found 100 posts. Total collected: 12600\n--- Fetching for 2023-07-01 to 2023-07-31 ---\nFetching r/depression...\n...found 100 posts. Total collected: 12700\nFetching r/mentalhealth...\n...found 100 posts. Total collected: 12800\nFetching r/Anxiety...\n...found 100 posts. Total collected: 12900\n--- Fetching for 2023-08-01 to 2023-08-31 ---\nFetching r/depression...\n...found 100 posts. Total collected: 13000\nFetching r/mentalhealth...\n...found 100 posts. Total collected: 13100\nFetching r/Anxiety...\n...found 100 posts. Total collected: 13200\n--- Fetching for 2023-09-01 to 2023-09-30 ---\nFetching r/depression...\n...found 100 posts. Total collected: 13300\nFetching r/mentalhealth...\n...found 100 posts. Total collected: 13400\nFetching r/Anxiety...\n...found 100 posts. Total collected: 13500\n--- Fetching for 2023-10-01 to 2023-10-31 ---\nFetching r/depression...\n...found 100 posts. Total collected: 13600\nFetching r/mentalhealth...\n...found 100 posts. Total collected: 13700\nFetching r/Anxiety...\n...found 100 posts. Total collected: 13800\n--- Fetching for 2023-11-01 to 2023-11-30 ---\nFetching r/depression...\n...found 100 posts. Total collected: 13900\nFetching r/mentalhealth...\n...found 100 posts. Total collected: 14000\nFetching r/Anxiety...\n...found 100 posts. Total collected: 14100\n--- Fetching for 2023-12-01 to 2023-12-31 ---\nFetching r/depression...\n...found 100 posts. Total collected: 14200\nFetching r/mentalhealth...\n...found 100 posts. Total collected: 14300\nFetching r/Anxiety...\n...found 100 posts. Total collected: 14400\n--- Fetching for 2024-01-01 to 2024-01-31 ---\nFetching r/depression...\n...found 100 posts. Total collected: 14500\nFetching r/mentalhealth...\n...found 100 posts. Total collected: 14600\nFetching r/Anxiety...\n...found 100 posts. Total collected: 14700\n--- Fetching for 2024-02-01 to 2024-02-29 ---\nFetching r/depression...\n...found 100 posts. Total collected: 14800\nFetching r/mentalhealth...\n...found 100 posts. Total collected: 14900\nFetching r/Anxiety...\n...found 100 posts. Total collected: 15000\n--- Fetching for 2024-03-01 to 2024-03-31 ---\nFetching r/depression...\n...found 100 posts. Total collected: 15100\nFetching r/mentalhealth...\n...found 100 posts. Total collected: 15200\nFetching r/Anxiety...\n...found 100 posts. Total collected: 15300\n--- Fetching for 2024-04-01 to 2024-04-30 ---\nFetching r/depression...\n...found 100 posts. Total collected: 15400\nFetching r/mentalhealth...\n...found 100 posts. Total collected: 15500\nFetching r/Anxiety...\n...found 100 posts. Total collected: 15600\n--- Fetching for 2024-05-01 to 2024-05-31 ---\nFetching r/depression...\n...found 100 posts. Total collected: 15700\nFetching r/mentalhealth...\n...found 100 posts. Total collected: 15800\nFetching r/Anxiety...\n...found 100 posts. Total collected: 15900\n--- Fetching for 2024-06-01 to 2024-06-30 ---\nFetching r/depression...\n...found 100 posts. Total collected: 16000\nFetching r/mentalhealth...\n...found 100 posts. Total collected: 16100\nFetching r/Anxiety...\n...found 100 posts. Total collected: 16200\n--- Fetching for 2024-07-01 to 2024-07-31 ---\nFetching r/depression...\n...found 100 posts. Total collected: 16300\nFetching r/mentalhealth...\n...found 100 posts. Total collected: 16400\nFetching r/Anxiety...\n...found 100 posts. Total collected: 16500\n--- Fetching for 2024-08-01 to 2024-08-31 ---\nFetching r/depression...\n...found 100 posts. Total collected: 16600\nFetching r/mentalhealth...\n...found 100 posts. Total collected: 16700\nFetching r/Anxiety...\n...found 100 posts. Total collected: 16800\n--- Fetching for 2024-09-01 to 2024-09-30 ---\nFetching r/depression...\n...found 100 posts. Total collected: 16900\nFetching r/mentalhealth...\n...found 100 posts. Total collected: 17000\nFetching r/Anxiety...\n...found 100 posts. Total collected: 17100\n--- Fetching for 2024-10-01 to 2024-10-31 ---\nFetching r/depression...\n...found 100 posts. Total collected: 17200\nFetching r/mentalhealth...\n...found 100 posts. Total collected: 17300\nFetching r/Anxiety...\n...found 100 posts. Total collected: 17400\n--- Fetching for 2024-11-01 to 2024-11-30 ---\nFetching r/depression...\n...found 100 posts. Total collected: 17500\nFetching r/mentalhealth...\n...found 100 posts. Total collected: 17600\nFetching r/Anxiety...\n...found 100 posts. Total collected: 17700\n--- Fetching for 2024-12-01 to 2024-12-31 ---\nFetching r/depression...\n...found 100 posts. Total collected: 17800\nFetching r/mentalhealth...\n...found 100 posts. Total collected: 17900\nFetching r/Anxiety...\n...found 100 posts. Total collected: 18000\n--- Fetching for 2025-01-01 to 2025-01-31 ---\nFetching r/depression...\n...found 100 posts. Total collected: 18100\nFetching r/mentalhealth...\n...found 100 posts. Total collected: 18200\nFetching r/Anxiety...\n...found 100 posts. Total collected: 18300\n--- Fetching for 2025-02-01 to 2025-02-28 ---\nFetching r/depression...\n...found 100 posts. Total collected: 18400\nFetching r/mentalhealth...\n...found 100 posts. Total collected: 18500\nFetching r/Anxiety...\n...found 100 posts. Total collected: 18600\n--- Fetching for 2025-03-01 to 2025-03-31 ---\nFetching r/depression...\n...found 100 posts. Total collected: 18700\nFetching r/mentalhealth...\n...found 100 posts. Total collected: 18800\nFetching r/Anxiety...\n...found 100 posts. Total collected: 18900\n--- Fetching for 2025-04-01 to 2025-04-30 ---\nFetching r/depression...\n...found 100 posts. Total collected: 19000\nFetching r/mentalhealth...\n...found 100 posts. Total collected: 19100\nFetching r/Anxiety...\n...found 100 posts. Total collected: 19200\n--- Fetching for 2025-05-01 to 2025-05-31 ---\nFetching r/depression...\n...found 100 posts. Total collected: 19300\nFetching r/mentalhealth...\n...found 100 posts. Total collected: 19400\nFetching r/Anxiety...\n...found 100 posts. Total collected: 19500\n--- Fetching for 2025-06-01 to 2025-06-30 ---\nFetching r/depression...\n...found 0 posts. Total collected: 19500\nFetching r/mentalhealth...\n...found 0 posts. Total collected: 19500\nFetching r/Anxiety...\n...found 0 posts. Total collected: 19500\n--- Fetching for 2025-07-01 to 2025-07-31 ---\nFetching r/depression...\n...found 0 posts. Total collected: 19500\nFetching r/mentalhealth...\n...found 0 posts. Total collected: 19500\nFetching r/Anxiety...\n...found 0 posts. Total collected: 19500\n--- Fetching for 2025-08-01 to 2025-08-31 ---\nFetching r/depression...\n...found 0 posts. Total collected: 19500\nFetching r/mentalhealth...\n...found 0 posts. Total collected: 19500\nFetching r/Anxiety...\n...found 0 posts. Total collected: 19500\n--- Fetching for 2025-09-01 to 2025-09-30 ---\nFetching r/depression...\n...found 0 posts. Total collected: 19500\nFetching r/mentalhealth...\n...found 0 posts. Total collected: 19500\nFetching r/Anxiety...\n...found 0 posts. Total collected: 19500\n--- Fetching for 2025-10-01 to 2025-10-31 ---\nFetching r/depression...\n...found 0 posts. Total collected: 19500\nFetching r/mentalhealth...\n...found 0 posts. Total collected: 19500\nFetching r/Anxiety...\n...found 0 posts. Total collected: 19500\n--- Fetching for 2025-11-01 to 2025-11-04 ---\nFetching r/depression...\n...found 0 posts. Total collected: 19500\nFetching r/mentalhealth...\n...found 0 posts. Total collected: 19500\nFetching r/Anxiety...\n...found 0 posts. Total collected: 19500\n\n--- FINISHED ALL SEARCHES ---\nSuccessfully collected a total of 19500 posts.\n\n\nWhat does this data actually look like?\n\n\nCode\nall_posts[0]\n\n\n{'all_awardings': [],\n 'allow_live_comments': False,\n 'archived': False,\n 'author': 'bumblebeehoneycomb',\n 'author_created_utc': 1493589289,\n 'author_flair_background_color': None,\n 'author_flair_css_class': None,\n 'author_flair_richtext': [],\n 'author_flair_template_id': None,\n 'author_flair_text': None,\n 'author_flair_text_color': None,\n 'author_flair_type': 'text',\n 'author_fullname': 't2_17jnok',\n 'author_patreon_flair': False,\n 'author_premium': False,\n 'awarders': [],\n 'can_gild': True,\n 'can_mod_post': False,\n 'category': None,\n 'content_categories': None,\n 'contest_mode': False,\n 'created_utc': 1577836856,\n 'discussion_type': None,\n 'distinguished': None,\n 'domain': 'self.depression',\n 'edited': False,\n 'gilded': 0,\n 'gildings': {},\n 'hidden': False,\n 'id': 'eib0d8',\n 'is_crosspostable': True,\n 'is_meta': False,\n 'is_original_content': False,\n 'is_reddit_media_domain': False,\n 'is_robot_indexable': True,\n 'is_self': True,\n 'is_video': False,\n 'link_flair_background_color': '',\n 'link_flair_css_class': None,\n 'link_flair_richtext': [],\n 'link_flair_text': None,\n 'link_flair_text_color': 'dark',\n 'link_flair_type': 'text',\n 'locked': False,\n 'media': None,\n 'media_embed': {},\n 'media_only': False,\n 'no_follow': True,\n 'num_comments': 8,\n 'num_crossposts': 0,\n 'over_18': False,\n 'parent_whitelist_status': 'no_ads',\n 'permalink': '/r/depression/comments/eib0d8/hows_everyone_doing_on_this_new_years_eve/',\n 'pinned': False,\n 'pwls': 0,\n 'quarantine': False,\n 'removal_reason': None,\n 'removed_by': None,\n 'removed_by_category': None,\n 'retrieved_on': 1586941625,\n 'score': 5,\n 'secure_media': None,\n 'secure_media_embed': {},\n 'selftext': \"I know as well as everyone new years makes you think back and regret and wonder why you're still here one more year..\\n\\nWell i'm spending new years alone at home and I'm just wondering how everyone is doing and if you need a place to vent or talk this can be a safe place for you. \\n\\nHappy new year everyone. We made it and maybe things will get better.\",\n 'send_replies': True,\n 'spoiler': False,\n 'stickied': False,\n 'subreddit': 'depression',\n 'subreddit_id': 't5_2qqqf',\n 'subreddit_name_prefixed': 'r/depression',\n 'subreddit_subscribers': 623857,\n 'subreddit_type': 'public',\n 'suggested_sort': 'confidence',\n 'thumbnail': 'self',\n 'thumbnail_height': None,\n 'thumbnail_width': None,\n 'title': 'Hows everyone doing on this new years eve?',\n 'total_awards_received': 0,\n 'treatment_tags': [],\n 'url': 'https://www.reddit.com/r/depression/comments/eib0d8/hows_everyone_doing_on_this_new_years_eve/',\n 'whitelist_status': 'no_ads',\n 'wls': 0}\n\n\nLet’s clean this data so we can work for it later and only pull the relevant information we need.\n\n\nCode\ncleaned_data = []\n\nfor post in all_posts:\n    cleaned_data.append({\n        \"title\": post[\"title\"],\n        \"content\": post[\"selftext\"],\n        \"date\": post[\"created_utc\"],\n        \"subreddit\": post[\"subreddit\"],\n        \"link\": post[\"url\"]\n    })\n\n\n\n\nCode\nimport csv\n\n\nWrite this data (JSON) to a CSV file.\n\n\nCode\nfieldnames = [\"title\", \"content\", \"date\", \"subreddit\", \"link\"]\n\nwith open(\"reddit_data.csv\", \"w\", newline='') as csvfile:\n    writer = csv.DictWriter(csvfile, fieldnames=fieldnames)\n    \n    writer.writeheader()\n    \n    writer.writerows(cleaned_data)",
    "crumbs": [
      "View on GitHub",
      "Collection (5 Years)"
    ]
  },
  {
    "objectID": "notebooks/covid_specific_data.html",
    "href": "notebooks/covid_specific_data.html",
    "title": "Analyzing specific stressors and COVID-related vocab",
    "section": "",
    "text": "Code\ncovid_terms = {\n    'pandemic_terms': ['pandemic', 'covid', 'coronavirus', 'virus', 'outbreak'],\n    'lockdown_terms': ['lockdown', 'quarantine', 'isolation', 'isolating', 'isolated', 'alone'],\n    'remote_terms': ['remote', 'virtual', 'online', 'zoom', 'video', 'wfh', 'work from home'],\n    'health_safety': ['mask', 'vaccine', 'vaccinated', 'social distancing', 'distance'],\n    'therapy_terms': ['therapist', 'therapy', 'counseling', 'telehealth', 'virtual therapy', 'online therapy']\n}\n\n\n\n\nCode\ndef track_specific_terms(df, terms_dict, text_column=\"full_text\"):\n    results = {}\n    \n    for period in df['covid_period'].unique():\n        period_data = df[df['covid_period'] == period]\n        period_results = {}\n        \n        # get all text for i-th period\n        all_text = ' '.join(period_data[text_column].fillna('').str.lower())\n        total_words = len(all_text.split())\n        \n        for category, terms in terms_dict.items():\n            category_count = 0\n            term_counts = {}\n            \n            for term in terms:\n                count = all_text.count(term.lower())\n                term_counts[term] = count\n                category_count += count\n                \n            period_results[category] = {\n                \"total_count\": category_count,\n                \"frequency_per_1000\": (category_count / total_words * 1000) if total_words &gt; 0 else 0,\n                'term_breakdown': term_counts\n            }\n        \n        results[period] = period_results\n        \n    return results\n\n\nThis function counts occurrences of specific terms within each COVID period. It calculates both raw counts and normalized frequencies (per 1000 words), which allows fair comparison across periods with different post volumes. The function joins all text for a period, counts each term, and provides detailed breakdowns.\n\n\nCode\n%store -r reddit_sent_df\n\n\n\n\nCode\ncovid_tracking = track_specific_terms(reddit_sent_df, covid_terms)\n\n\n\n\nCode\nprint(\"COVID-Related Term Frequencies by Period\")\nprint(\"=\" * 60)\n\nfor period in ['Pre-COVID', 'During COVID', 'Post-COVID']:\n    if period in covid_tracking:\n        print(f\"\\n{period}:\")\n        for category, data in covid_tracking[period].items():\n            print(f\"\\n  {category}:\")\n            print(f\"    Total mentions: {data['total_count']}\")\n            print(f\"    Per 1000 words: {data['frequency_per_1000']:.2f}\")\n            \n            # Show top terms in category\n            top_terms = sorted(data['term_breakdown'].items(), key=lambda x: x[1], reverse=True)[:3]\n            if any(count &gt; 0 for _, count in top_terms):\n                print(\"    Most mentioned:\")\n                for term, count in top_terms:\n                    if count &gt; 0:\n                        print(f\"      - {term}: {count}\")\n\n\nCOVID-Related Term Frequencies by Period\n============================================================\n\nPre-COVID:\n\n  pandemic_terms:\n    Total mentions: 3\n    Per 1000 words: 0.05\n    Most mentioned:\n      - virus: 2\n      - coronavirus: 1\n\n  lockdown_terms:\n    Total mentions: 91\n    Per 1000 words: 1.47\n    Most mentioned:\n      - alone: 81\n      - isolated: 8\n      - isolation: 1\n\n  remote_terms:\n    Total mentions: 26\n    Per 1000 words: 0.42\n    Most mentioned:\n      - online: 14\n      - video: 10\n      - remote: 1\n\n  health_safety:\n    Total mentions: 5\n    Per 1000 words: 0.08\n    Most mentioned:\n      - vaccine: 2\n      - distance: 2\n      - mask: 1\n\n  therapy_terms:\n    Total mentions: 69\n    Per 1000 words: 1.12\n    Most mentioned:\n      - therapist: 38\n      - therapy: 30\n      - counseling: 1\n\nDuring COVID:\n\n  pandemic_terms:\n    Total mentions: 661\n    Per 1000 words: 0.56\n    Most mentioned:\n      - covid: 378\n      - pandemic: 180\n      - virus: 71\n\n  lockdown_terms:\n    Total mentions: 1298\n    Per 1000 words: 1.09\n    Most mentioned:\n      - alone: 978\n      - quarantine: 88\n      - lockdown: 70\n\n  remote_terms:\n    Total mentions: 723\n    Per 1000 words: 0.61\n    Most mentioned:\n      - video: 300\n      - online: 297\n      - remote: 45\n\n  health_safety:\n    Total mentions: 200\n    Per 1000 words: 0.17\n    Most mentioned:\n      - mask: 82\n      - distance: 64\n      - vaccine: 37\n\n  therapy_terms:\n    Total mentions: 1523\n    Per 1000 words: 1.28\n    Most mentioned:\n      - therapy: 719\n      - therapist: 643\n      - counseling: 154\n\nPost-COVID:\n\n  pandemic_terms:\n    Total mentions: 147\n    Per 1000 words: 0.16\n    Most mentioned:\n      - covid: 97\n      - pandemic: 35\n      - virus: 13\n\n  lockdown_terms:\n    Total mentions: 897\n    Per 1000 words: 0.96\n    Most mentioned:\n      - alone: 762\n      - isolated: 62\n      - isolation: 33\n\n  remote_terms:\n    Total mentions: 453\n    Per 1000 words: 0.49\n    Most mentioned:\n      - video: 205\n      - online: 186\n      - remote: 26\n\n  health_safety:\n    Total mentions: 106\n    Per 1000 words: 0.11\n    Most mentioned:\n      - distance: 51\n      - mask: 50\n      - vaccine: 4\n\n  therapy_terms:\n    Total mentions: 1085\n    Per 1000 words: 1.16\n    Most mentioned:\n      - therapy: 626\n      - therapist: 421\n      - counseling: 28\n\n\n\n\nCode\n# Specific Stressor Analysis\nstressor_terms = {\n    \"job_related\": [\n        \"job\",\n        \"work\",\n        \"unemployed\",\n        \"fired\",\n        \"laid off\",\n        \"career\",\n        \"boss\",\n        \"workplace\",\n        \"employment\",\n    ],\n    \"social_isolation\": [\n        \"alone\",\n        \"lonely\",\n        \"isolated\",\n        \"friends\",\n        \"social\",\n        \"nobody\",\n        \"anyone\",\n        \"loneliness\",\n    ],\n    \"health_anxiety\": [\n        \"health\",\n        \"sick\",\n        \"illness\",\n        \"symptoms\",\n        \"doctor\",\n        \"medical\",\n        \"hospital\",\n        \"disease\",\n    ],\n    \"financial_stress\": [\n        \"money\",\n        \"bills\",\n        \"rent\",\n        \"financial\",\n        \"afford\",\n        \"broke\",\n        \"debt\",\n        \"pay\",\n    ],\n    \"family_issues\": [\n        \"family\",\n        \"parents\",\n        \"mother\",\n        \"father\",\n        \"mom\",\n        \"dad\",\n        \"siblings\",\n        \"relationship\",\n    ],\n    \"academic_stress\": [\n        \"school\",\n        \"college\",\n        \"university\",\n        \"exam\",\n        \"study\",\n        \"student\",\n        \"class\",\n        \"grade\",\n    ],\n    # Below are categories made from doing a machine learning clustering algorithm\n    # trying to find patterns in the text that I couldn't have come up with\n    \"health_anxiety\": [\n        \"heart\",\n        \"symptoms\",\n        \"panic attack\",\n        \"panic attacks\",\n        \"scared\",\n        \"pain\",\n        \"health\",\n        \"anxious\",\n        \"attack\",\n    ],\n    \"work_stress\": [\"job\", \"home\", \"house\", \"wfh\", \"remote\", \"work\"],\n    \"school_stress\": [\n        \"school\",\n        \"parents\",\n        \"mom\",\n        \"dad\",\n        \"remote school\",\n        \"class\",\n        \"online class\",\n    ],\n    \"burnout\": [\"tired\", \"anymore\", \"hate\", \"exhausted\", \"fucking tired\", \"end\"],\n}\n\n\n\n\nCode\nstressor_tracking = track_specific_terms(reddit_sent_df, stressor_terms)\n\n\n\n\nCode\nprint(\"\\n\\nStressor-Related Term Frequencies by Period\")\nprint(\"=\" * 60)\n\nfor period in ['Pre-COVID', 'During COVID', 'Post-COVID']:\n    if period in stressor_tracking:\n        print(f\"\\n{period}:\")\n        for category, data in stressor_tracking[period].items():\n            print(f\"\\n  {category}:\")\n            print(f\"    Total mentions: {data['total_count']}\")\n            print(f\"    Per 1000 words: {data['frequency_per_1000']:.2f}\")\n            \n            # Show top terms in category\n            top_terms = sorted(data['term_breakdown'].items(), key=lambda x: x[1], reverse=True)[:3]\n            if any(count &gt; 0 for _, count in top_terms):\n                print(\"    Most mentioned:\")\n                for term, count in top_terms:\n                    if count &gt; 0:\n                        print(f\"      - {term}: {count}\")\n\n\n\n\nStressor-Related Term Frequencies by Period\n============================================================\n\nPre-COVID:\n\n  job_related:\n    Total mentions: 264\n    Per 1000 words: 4.27\n    Most mentioned:\n      - work: 165\n      - job: 79\n      - career: 8\n\n  social_isolation:\n    Total mentions: 398\n    Per 1000 words: 6.43\n    Most mentioned:\n      - friends: 132\n      - anyone: 102\n      - alone: 81\n\n  health_anxiety:\n    Total mentions: 370\n    Per 1000 words: 5.98\n    Most mentioned:\n      - health: 78\n      - attack: 55\n      - scared: 49\n\n  financial_stress:\n    Total mentions: 185\n    Per 1000 words: 2.99\n    Most mentioned:\n      - rent: 96\n      - money: 30\n      - pay: 24\n\n  family_issues:\n    Total mentions: 352\n    Per 1000 words: 5.69\n    Most mentioned:\n      - family: 87\n      - mom: 78\n      - relationship: 47\n\n  academic_stress:\n    Total mentions: 174\n    Per 1000 words: 2.81\n    Most mentioned:\n      - school: 81\n      - class: 20\n      - college: 17\n\n  work_stress:\n    Total mentions: 354\n    Per 1000 words: 5.72\n    Most mentioned:\n      - work: 165\n      - job: 79\n      - home: 66\n\n  school_stress:\n    Total mentions: 253\n    Per 1000 words: 4.09\n    Most mentioned:\n      - school: 81\n      - mom: 78\n      - parents: 46\n\n  burnout:\n    Total mentions: 589\n    Per 1000 words: 9.52\n    Most mentioned:\n      - end: 415\n      - hate: 75\n      - anymore: 66\n\nDuring COVID:\n\n  job_related:\n    Total mentions: 4756\n    Per 1000 words: 4.01\n    Most mentioned:\n      - work: 3217\n      - job: 1195\n      - career: 132\n\n  social_isolation:\n    Total mentions: 6633\n    Per 1000 words: 5.59\n    Most mentioned:\n      - friends: 2158\n      - anyone: 2126\n      - alone: 978\n\n  health_anxiety:\n    Total mentions: 7629\n    Per 1000 words: 6.43\n    Most mentioned:\n      - health: 1515\n      - anxious: 1066\n      - attack: 1048\n\n  financial_stress:\n    Total mentions: 3707\n    Per 1000 words: 3.13\n    Most mentioned:\n      - rent: 2271\n      - money: 410\n      - broke: 379\n\n  family_issues:\n    Total mentions: 5533\n    Per 1000 words: 4.67\n    Most mentioned:\n      - mom: 1307\n      - family: 1262\n      - parents: 942\n\n  academic_stress:\n    Total mentions: 3588\n    Per 1000 words: 3.03\n    Most mentioned:\n      - school: 1456\n      - college: 517\n      - class: 467\n\n  work_stress:\n    Total mentions: 6149\n    Per 1000 words: 5.19\n    Most mentioned:\n      - work: 3217\n      - job: 1195\n      - home: 1059\n\n  school_stress:\n    Total mentions: 4738\n    Per 1000 words: 4.00\n    Most mentioned:\n      - school: 1456\n      - mom: 1307\n      - parents: 942\n\n  burnout:\n    Total mentions: 10167\n    Per 1000 words: 8.57\n    Most mentioned:\n      - end: 6449\n      - hate: 1485\n      - anymore: 1247\n\nPost-COVID:\n\n  job_related:\n    Total mentions: 3787\n    Per 1000 words: 4.06\n    Most mentioned:\n      - work: 2512\n      - job: 1017\n      - career: 97\n\n  social_isolation:\n    Total mentions: 4932\n    Per 1000 words: 5.29\n    Most mentioned:\n      - anyone: 1655\n      - friends: 1465\n      - alone: 762\n\n  health_anxiety:\n    Total mentions: 6794\n    Per 1000 words: 7.29\n    Most mentioned:\n      - health: 1197\n      - attack: 956\n      - scared: 810\n\n  financial_stress:\n    Total mentions: 2751\n    Per 1000 words: 2.95\n    Most mentioned:\n      - rent: 1615\n      - money: 311\n      - broke: 292\n\n  family_issues:\n    Total mentions: 4386\n    Per 1000 words: 4.70\n    Most mentioned:\n      - family: 1066\n      - mom: 1008\n      - parents: 644\n\n  academic_stress:\n    Total mentions: 2547\n    Per 1000 words: 2.73\n    Most mentioned:\n      - school: 988\n      - college: 405\n      - class: 335\n\n  work_stress:\n    Total mentions: 4892\n    Per 1000 words: 5.25\n    Most mentioned:\n      - work: 2512\n      - job: 1017\n      - home: 835\n\n  school_stress:\n    Total mentions: 3450\n    Per 1000 words: 3.70\n    Most mentioned:\n      - mom: 1008\n      - school: 988\n      - parents: 644\n\n  burnout:\n    Total mentions: 8314\n    Per 1000 words: 8.92\n    Most mentioned:\n      - end: 5314\n      - hate: 1164\n      - anymore: 989\n\n\n\n\n\n\nCode\nimport pandas as pd\n\ncategories = list(stressor_terms.keys())\nperiods = [\"Pre-COVID\", \"During COVID\", \"Post-COVID\"]\nperiod_order = [\"Pre-COVID\", \"During COVID\", \"Post-COVID\"] # For sorting\n\n# --- Create a \"tidy\" DataFrame ---\ndata_for_df = []\nfor period in periods:\n    if period in stressor_tracking:\n        for category in categories:\n            # Get the frequency, defaulting to 0 if not found\n            freq = stressor_tracking[period].get(category, {}).get(\"frequency_per_1000\", 0)\n            \n            data_for_df.append({\n                \"category\": category,\n                \"period\": period,\n                \"frequency\": freq\n            })\n\n# Create the DataFrame\ndf = pd.DataFrame(data_for_df)\n\n\n\n\nCode\nimport plotly.express as px\nimport plotly.io as pio\npio.renderers.default = \"notebook\"\n\nfig = px.bar(\n    df,\n    x=\"category\",\n    y=\"frequency\",\n    color=\"period\",\n    barmode=\"group\",\n    \n    title=\"Stressor Mentions Across COVID Periods\",\n    labels={\n        \"category\": \"Stressor Category\",\n        \"frequency\": \"Frequency per 1000 words\",\n        \"period\": \"COVID Period\"  # This becomes the legend title\n    },\n    \n    category_orders={\"period\": period_order}\n)\n\nfig.update_layout(\n    width=1200,\n    height=600,\n    xaxis_tickangle=45,\n    yaxis_gridcolor='rgba(0,0,0,0.1)',\n    margin=dict(b=120)\n)\n\nfig.show()\n\n\n        \n        \n        \n\n\n                                                    \n\n\nThis creates an interactive grouped bar chart using Plotly, allowing comparison of stressor frequencies across the three periods. Each stressor category has three bars (one per period), making visual comparison easy.",
    "crumbs": [
      "View on GitHub",
      "COVID Stressor Analysis"
    ]
  },
  {
    "objectID": "notebooks/covid_specific_data.html#visualize-changes-in-stressors-over-time",
    "href": "notebooks/covid_specific_data.html#visualize-changes-in-stressors-over-time",
    "title": "Analyzing specific stressors and COVID-related vocab",
    "section": "",
    "text": "Code\nimport pandas as pd\n\ncategories = list(stressor_terms.keys())\nperiods = [\"Pre-COVID\", \"During COVID\", \"Post-COVID\"]\nperiod_order = [\"Pre-COVID\", \"During COVID\", \"Post-COVID\"] # For sorting\n\n# --- Create a \"tidy\" DataFrame ---\ndata_for_df = []\nfor period in periods:\n    if period in stressor_tracking:\n        for category in categories:\n            # Get the frequency, defaulting to 0 if not found\n            freq = stressor_tracking[period].get(category, {}).get(\"frequency_per_1000\", 0)\n            \n            data_for_df.append({\n                \"category\": category,\n                \"period\": period,\n                \"frequency\": freq\n            })\n\n# Create the DataFrame\ndf = pd.DataFrame(data_for_df)\n\n\n\n\nCode\nimport plotly.express as px\nimport plotly.io as pio\npio.renderers.default = \"notebook\"\n\nfig = px.bar(\n    df,\n    x=\"category\",\n    y=\"frequency\",\n    color=\"period\",\n    barmode=\"group\",\n    \n    title=\"Stressor Mentions Across COVID Periods\",\n    labels={\n        \"category\": \"Stressor Category\",\n        \"frequency\": \"Frequency per 1000 words\",\n        \"period\": \"COVID Period\"  # This becomes the legend title\n    },\n    \n    category_orders={\"period\": period_order}\n)\n\nfig.update_layout(\n    width=1200,\n    height=600,\n    xaxis_tickangle=45,\n    yaxis_gridcolor='rgba(0,0,0,0.1)',\n    margin=dict(b=120)\n)\n\nfig.show()\n\n\n        \n        \n        \n\n\n                                                    \n\n\nThis creates an interactive grouped bar chart using Plotly, allowing comparison of stressor frequencies across the three periods. Each stressor category has three bars (one per period), making visual comparison easy.",
    "crumbs": [
      "View on GitHub",
      "COVID Stressor Analysis"
    ]
  },
  {
    "objectID": "notebooks/data_collecting.html",
    "href": "notebooks/data_collecting.html",
    "title": "Data Collection",
    "section": "",
    "text": "Data Collection\nThis notebook will be dedicated to actually collecting the Reddit data from subreddits\nData will include the title, timestamp, subreddit, and the content of the post\n\n\nCode\nimport praw\n\nreddit = praw.Reddit(\n    client_id=\"SoV0Gu10OjPamWDAoBJg7w\",\n    client_secret=\"eMguu0pW3JqSwmzNi2-VgZ5lEk8atw\",\n    user_agent=\"GLB HEALTH PROJECT\"\n)\n\n\n\n\nCode\nraw_posts = []\n\nfor submission in reddit.subreddit(\"depression\").new(limit=None):\n    raw_posts.append(submission)\n\n\n\n\nCode\nraw_posts[0].selftext\n\n\n'For context I’m a grade 12 student and recently my chronic depression has gotten way worse. I hate waking up everyday and school is a struggle. My marks are slipping and I might not get into the university program I want. Even if I did I wouldn’t wanna go. All I wanna do is lay in bed and not talk to anyone. I’m scared for my (doubtful) future. Yes I think about suicide everyday. But what if I don’t do it? I don’t have anything left. Why should I spend 4 hours studying everyday if I’m just gonna end up dying by suicide. But if I don’t study, and decide to not die, I won’t have anything left for my studies or future. I’m scared and honestly at a really dark confusing place right now. '\n\n\n\n\nCode\nlen(raw_posts)\n\n\n991\n\n\nSeems like there .top() will default at 100 posts, this should be more than enough considering we will look at multiple subreddits\n\n\nCode\ndef extract_data(subred: str):\n    posts = []\n    \n    for submission in reddit.subreddit(subred).top():\n        posts.append({\n            \"title\": submission.title,\n            \"content\": submission.selftext,\n            \"date\": submission.created_utc,\n            \"subreddit\": subred, \n        })\n        \n    return posts\n\n\n\n\nCode\nsubreddits = [\"depression\", \"Anxiety\", \"mentalhealth\"]\n\nraw_data = []\n\nfor sub in subreddits:\n    raw_data.append(extract_data(sub))\n    \n\n\n\n\nCode\nflat_raw_data = [item for sublist in raw_data for item in sublist]\n\n\n\n\nCode\nlen(flat_raw_data)\n\nflat_raw_data[0]\n\n\n{'title': 'Shout out to the particular hell that is functional depression.',\n 'content': 'This is me. Don’t get me wrong, it’s better than don’t-leave-my-bed-for-a-week depression. I am grateful I can be an independent person. But there is something uniquely horrible about being able to go to work every day, occasionally clean up after yourself, pay your bills, generally put yourself together enough to look like a human being... but that’s it. Nothing else. No social life. No hobbies. Constantly battling your mind. And being absolutely fucking exhausted all the time.',\n 'date': 1563090820.0,\n 'subreddit': 'depression'}\n\n\nYup, looks right, 3 subreddits, 100 posts for each\n\n\nCode\nimport csv\n\nfieldnames = [\"title\", \"content\", \"date\", \"subreddit\"]\n\nwith open(\"reddit_data.csv\", \"w\", newline='') as csvfile:\n    writer = csv.DictWriter(csvfile, fieldnames=fieldnames)\n    \n    writer.writeheader()\n    \n    writer.writerows(flat_raw_data)"
  },
  {
    "objectID": "notebooks/topic_modeling.html",
    "href": "notebooks/topic_modeling.html",
    "title": "",
    "section": "",
    "text": "CodeShow All CodeHide All Code\nComparing Topic Models for Each Period\nCode\nimport pandas\nimport re\nimport nltk\nfrom nltk.corpus import stopwords\nfrom sklearn.feature_extraction.text import CountVectorizer\nfrom sklearn.decomposition import LatentDirichletAllocation\nimport numpy as np\nCode\n%store -r reddit_sent_df\nCode\nnltk.data.find(\"corpora/stopwords\")\n\n\nFileSystemPathPointer('/home/beherya/nltk_data/corpora/stopwords')\nThis verifies that the stopwords corpus is installed and returns its location. Interpretation: The stopwords are successfully located, so the code can proceed to use them for text cleaning.\nCode\nbase_stopwords = set(stopwords.words(\"english\"))\nCode\ncustom_stopwords = set([\n    'like', 'get', 'dont', 'im', 'would', 'really', 'one', 'people',\n    'time', 'know', 'feel', 'even', 'go', 'want', 'think', 'much',\n    'life', 'day', 'days', 'years', 'year', 'something', 'nothing',\n    'got', 'make', 'feeling', 'going', 'things', 'way', 'work',\n    'help', 'cant', 'need', 'see', 'friends', 'family', 'ive', 'anyone',\n    'anything', 'always', 'else', 'getting', 'started'\n])\n\nfull_stop_words = base_stopwords.union(custom_stopwords)\nThis creates a comprehensive stopword list by combining standard English stopwords with custom ones. Custom stopwords are common words that appear frequently in conversational text but don’t carry specific meaning (like “like”, “really”, “things”). These will be removed from the text to focus on more meaningful content words.\nCode\ndef preprocess_text(text):\n    if not isinstance(text, str):\n        return \"\"\n\n    # Remove URLs\n    text = re.sub(r'http\\S+|www\\S+', '', text)\n    # Remove punctuation and numbers\n    text = re.sub(r'[^a-zA-Z\\s]', '', text)\n    # Convert to lowercase\n    text = text.lower()\n    return text\nThis function cleans text by removing URLs, stripping out punctuation and numbers, and converting everything to lowercase. This standardizes the text for analysis.\nCode\ndef display_topics(df, text_col, title, n_topics=5, n_top_words=10):\n    \"\"\"\n    Runs and prints topic models for a given DataFrame.\n    \"\"\"\n    print(\"\\n\" + \"=\"*50)\n    print(f\" {title} (n={len(df)} posts) \")\n    print(\"=\"*50)\n\n    if len(df) &lt; n_topics:\n        print(f\"Not enough documents to model {n_topics} topics. Skipping.\")\n        return\n\n    # 1. Vectorize: Convert text to a word-count matrix\n    # We apply our preprocessing and stopword removal here\n    vectorizer = CountVectorizer(\n        preprocessor=preprocess_text,\n        stop_words=list(full_stop_words),\n        max_df=0.9,  # Ignore words in &gt; 90% of docs\n        min_df=10,   # Ignore words in &lt; 10 docs\n        ngram_range=(1, 1) # Only use single words\n    )\n\n    try:\n        dtm = vectorizer.fit_transform(df[text_col])\n    except ValueError as e:\n        print(f\"Error vectorizing text (maybe all words were stopwords?): {e}\")\n        return\n\n    # 2. Model: Run Latent Dirichlet Allocation\n    lda = LatentDirichletAllocation(\n        n_components=n_topics,\n        random_state=42, # For reproducible results\n        n_jobs=-1\n    )\n    lda.fit(dtm)\n\n    # 3. Display: Print the top words for each topic\n    feature_names = vectorizer.get_feature_names_out()\n\n    for topic_idx, topic in enumerate(lda.components_):\n        # Get the indices of the top words\n        top_words_indices = topic.argsort()[:-n_top_words - 1:-1]\n        # Get the words themselves\n        top_words = [feature_names[i] for i in top_words_indices]\n        print(f\"Topic {topic_idx + 1}: {' '.join(top_words)}\")\nThis is the main function that performs topic modeling using Latent Dirichlet Allocation (LDA). It:\nUses CountVectorizer to convert text into a document-term matrix (word counts) Filters out very common words (appearing in &gt;90% of documents) and very rare words (appearing in &lt;10 documents)\nFits an LDA model with 5 topics\nExtracts and displays the top 10 words for each topic\nThe parameters random_state=42 ensures reproducibility, and n_jobs=-1 uses all CPU cores for faster processing.\nCode\npre_covid_df = reddit_sent_df[reddit_sent_df[\"covid_period\"] == \"Pre-COVID\"]\nduring_covid_df = reddit_sent_df[reddit_sent_df[\"covid_period\"] == \"During COVID\"]\npost_covid_df = reddit_sent_df[reddit_sent_df[\"covid_period\"] == \"Post-COVID\"]\nThis splits the Reddit dataset into three time periods to compare how topics changed before, during, and after COVID-19.\nCode\ndisplay_topics(pre_covid_df, 'full_text', title=\"Pre-COVID Topics\")\ndisplay_topics(during_covid_df, 'full_text', title=\"During-COVID Topics\")\ndisplay_topics(post_covid_df, 'full_text', title=\"Post-COVID Topics\")\n\n\n\n==================================================\n Pre-COVID Topics (n=576 posts) \n==================================================\nTopic 1: fucking everyone alone someone love nye never hate fuck many\nTopic 2: mental anxiety health didnt first school felt back night since\nTopic 3: anxiety bad good person happy anxious someone everything tired talk\nTopic 4: new happy alone everyone better everything anxiety anymore hope shit\nTopic 5: anxiety panic attack sleep never someone told depression job pain\n\n==================================================\n During-COVID Topics (n=10517 posts) \n==================================================\nTopic 1: anxiety anxious panic also sleep attack bad attacks take heart\nTopic 2: job mental health back home since house didnt last told\nTopic 3: someone depression thoughts mental talk person love self good thing\nTopic 4: anymore everything hate never tired every happy fucking better good\nTopic 5: school didnt never deleted user friend parents mom said told\n\n==================================================\n Post-COVID Topics (n=6218 posts) \n==================================================\nTopic 1: end talk didnt someone say said friend never thing told\nTopic 2: job never mental live love mom parents health school hate\nTopic 3: depression better anymore everything thoughts good bad happy tired never\nTopic 4: anxiety heart symptoms sleep also pain panic back scared health\nTopic 5: anxiety panic home also anxious take night attacks back attack\n\n\n/home/beherya/code/schooling/northwestern-senior/glb_hlth_final_project/env/lib/python3.13/site-packages/sklearn/feature_extraction/text.py:402: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['arent', 'couldnt', 'didnt', 'doesnt', 'hadnt', 'hasnt', 'havent', 'hed', 'hell', 'hes', 'id', 'ill', 'isnt', 'itd', 'itll', 'mightnt', 'mustnt', 'neednt', 'shant', 'shed', 'shell', 'shes', 'shouldnt', 'shouldve', 'thatll', 'theyd', 'theyll', 'theyre', 'theyve', 'wasnt', 'wed', 'well', 'werent', 'weve', 'wont', 'wouldnt', 'youd', 'youll', 'youre', 'youve'] not in stop_words.\n  warnings.warn(",
    "crumbs": [
      "View on GitHub",
      "Topic Modeling"
    ]
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "",
    "section": "",
    "text": "Code\n\n\n\n\n\nHow to Use",
    "crumbs": [
      "View on GitHub",
      "How to Use"
    ]
  },
  {
    "objectID": "notebooks/data_analysis.html",
    "href": "notebooks/data_analysis.html",
    "title": "Using VADER to perform simple sentiment analysis",
    "section": "",
    "text": "Using VADER to perform simple sentiment analysis\n\n\nCode\n%store -r reddit_df\nreddit_df = reddit_df\n\nimport pandas as pd\n\n\nThe compound score is a single, normalized number representing the overall sentiment of the text, ranging from -1 (most negative) to +1 (most positive). VADER calculates this by looking up each word’s sentiment (valence) in its dictionary, adjusting for things like capital letters (“GREAT”), punctuation (“!”), and modifiers (“very”). It then combines these individual scores and normalizes them to produce the final compound value.\n\n\nCode\nfrom vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer\nsenti_analyzer = SentimentIntensityAnalyzer()\n\n\n\nCombining the titles and the content of the posts for a “full text” column\n\n\nCode\nreddit_df[\"full_text\"] = reddit_df[\"title\"] + \"\\n\" + reddit_df[\"content\"]\n\n\nThis function finds the sentiment score of every single full post in the dataframe\n\n\nCode\ndef get_sentiment_scores(text):\n    if pd.isna(text) or text == '':\n        return {'neg': 0, 'neu': 0, 'pos': 0, 'compound': 0}\n    return senti_analyzer.polarity_scores(text)\n\nsentiment_scores = reddit_df['full_text'].apply(lambda x: get_sentiment_scores(x))\n\n\nAssign labels for each of the outputs of the VADER sentiment scores\n\n\nCode\nreddit_df[\"compound\"] = sentiment_scores.apply(lambda x: x[\"compound\"])\nreddit_df[\"positive\"] = sentiment_scores.apply(lambda x: x[\"pos\"])\nreddit_df[\"negative\"] = sentiment_scores.apply(lambda x: x[\"neg\"])\nreddit_df[\"neutral\"] = sentiment_scores.apply(lambda x: x[\"neu\"])\n\n\n\n\nCode\nreddit_df[\"year\"] = reddit_df[\"date\"].dt.year\n\n\nCategorize the data into buckets of different COVID periods\n\n\nCode\ndef assign_covid_period(date):\n    if date &lt; pd.Timestamp(\"2020-03-01\"):\n        return \"Pre-COVID\"\n    elif date &gt;= pd.Timestamp(\"2020-03-01\") and date &lt; pd.Timestamp(\"2023-05-01\"):\n        return \"During COVID\"\n    else:\n        return \"Post-COVID\"\n    \nreddit_df[\"covid_period\"] = reddit_df[\"date\"].apply(assign_covid_period)\n\n\n\n\nCode\nyearly_sentiment = reddit_df.groupby('year').agg({\n    'compound': 'mean',\n    'positive': 'mean',\n    'negative': 'mean',\n    'neutral': 'mean'\n}).round(4)\n\nprint(\"Average Sentiment Scores by Year\")\nprint(yearly_sentiment)\n\n\nAverage Sentiment Scores by Year\n      compound  positive  negative  neutral\nyear                                       \n2020   -0.1997    0.1113    0.1775   0.7112\n2021   -0.1710    0.0979    0.1558   0.7463\n2022   -0.1965    0.1066    0.1556   0.7378\n2023   -0.2497    0.1114    0.1573   0.7313\n2024   -0.2382    0.1056    0.1620   0.7324\n2025   -0.2772    0.1026    0.1616   0.7358\n\n\nHere, I’m grouping my sentiment data by year to see the long-term trends. I calculate the average for the compound, positive, negative, and neutral scores for each year.\nThe resulting table shows that the overall compound sentiment is consistently negative across all years, but it becomes even more negative from 2023 to 2025. This suggests that the language in these subreddits has grown more negative in the post-pandemic period compared to during the pandemic itself.\n\n\nCode\n# Group by COVID period\nperiod_statement = reddit_df.groupby('covid_period').agg({\n    'compound': ['mean', 'std', 'count'],\n    'negative': 'mean',\n    'positive': 'mean'\n}).round(4)\n\nprint(\"Sentiment by COVID Period\")\nprint(period_statement)\n\n\nSentiment by COVID Period\n             compound                negative positive\n                 mean     std  count     mean     mean\ncovid_period                                          \nDuring COVID  -0.1967  0.6350  10517   0.1624   0.1049\nPost-COVID    -0.2485  0.7024   6218   0.1600   0.1073\nPre-COVID     -0.1463  0.6532    576   0.1747   0.1142\n\n\nThis code groups all my posts into the ‘Pre-COVID’, ‘During COVID’, and ‘Post-COVID’ periods. For each period, it’s calculating the average compound, negative, and positive scores. It also gets the standard deviation (std) and post count for the main compound score to check for consistency and data volume.\nThis output clearly shows that the overall sentiment (compound score) became more negative during the pandemic (-0.1967) compared to the ‘Pre-COVID’ period (-0.1463). More importantly, the sentiment in the ‘Post-COVID’ period (-0.2485) is even more negative than it was during the pandemic. This suggests that the collective mental health discourse in these subreddits has not improved and has, in fact, trended further into negative territory.\n\n\nCode\nimport plotly.express as px\nimport plotly.io as pio\npio.renderers.default = \"notebook\"\n\n# Create the base line plot\nfig = px.line(\n    yearly_sentiment,\n    x=yearly_sentiment.index,\n    y=\"compound\",\n    markers=True,  # Creates the 'o' markers\n    text=\"compound\",  # Use the 'compound' column for text labels\n    # Set titles and labels\n    title=\"Reddit Mental Health Sentiment Over Time\",\n    labels={\n        \"index\": \"Year\",  # 'index' because we passed the index as x\n        \"compound\": \"Average Compound Sentiment Score\",\n    },\n)\n\n# Add the vertical 'COVID Start' line\nfig.add_vline(\n    x=2020,\n    line_dash=\"dash\",  # Replaces 'linestyle'\n    line_color=\"red\",  # Replaces 'color'\n    annotation_text=\"COVID Start\",  # This is Plotly's 'label'\n    annotation_position=\"top right\",\n)\n\n# Format the text labels on the points\nfig.update_traces(\n    textposition=\"bottom right\",  # Matches 'ha='center', va='bottom'\n    texttemplate=\"%{text:.3f}\",  # Formats the text like '{score:.3f}'\n)\n\n# Adjust layout size and grid (Plotly's grid is on by default)\nfig.update_layout(\n    width=1000,  # Roughly matches figsize=(10, 6)\n    height=600,\n    xaxis_gridcolor=\"rgba(0,0,0,0.1)\",  # Lighter grid, like 'alpha=0.3'\n    yaxis_gridcolor=\"rgba(0,0,0,0.1)\",\n)\n\nfig.show()\n\n\n        \n        \n        \n\n\n                                                    \n\n\nThe data seems to show that the pandemic’s impact on mental health wasn’t a temporary event that ended when the lockdowns lifted. Instead, it appears to have introduced or amplified long term negative scores. While the acute anxiety of the virus and lockdowns faded, they were replaced by chronic issues like economic inflation, job instability, and the stress of “returning to normal”, which created a new set of anxieties as we’ll later explore in this dataset.\nFurthermore, the prolonged social isolation may have caused lasting damage to social structures and individual well-being, leading to persistent loneliness and disconnection. The data suggests we are now living with the compounded consequences of the pandemic, which are proving to be just as, or even more, detrimental to collective mental health than the initial crisis itself.\n\n\nCode\n# Check data distribution\nprint(\"Posts per year:\")\nprint(reddit_df['year'].value_counts().sort_index())\n\n# Compare pre vs during COVID (if you have that data)\nprint(\"\\nAverage compound sentiment by period:\")\nfor period in ['Pre-COVID', 'During COVID', 'Post-COVID']:\n    data = reddit_df[reddit_df['covid_period'] == period]\n    if len(data) &gt; 0:\n        print(f\"{period}: {data['compound'].mean():.4f} (n={len(data)})\")\n\n\nPosts per year:\nyear\n2020    3536\n2021    3567\n2022    3159\n2023    2775\n2024    2993\n2025    1281\nName: count, dtype: int64\n\nAverage compound sentiment by period:\nPre-COVID: -0.1463 (n=576)\nDuring COVID: -0.1967 (n=10517)\nPost-COVID: -0.2485 (n=6218)\n\n\nI have a solid collection of posts, with thousands from each year between 2020 and 2024. The lower number for 2025 just means I ran the data collection part-way through that year.\nThe average sentiment was already negative Pre-COVID (-0.1463) but became significantly more negative During COVID (-0.1967). Most importantly, instead of recovering, the sentiment has grown even more negative in the Post-COVID period (-0.2485), suggesting that the pandemic has had a lasting and worsening impact on the mental health discourse in these subreddits.\n\n\nCode\nreddit_sent_df = reddit_df\n\n\n\n\nCode\n%store reddit_sent_df\n\n\nStored 'reddit_sent_df' (DataFrame)",
    "crumbs": [
      "View on GitHub",
      "Sentiment Analysis"
    ]
  },
  {
    "objectID": "index.html#project-overview",
    "href": "index.html#project-overview",
    "title": "",
    "section": "Project Overview",
    "text": "Project Overview\nThis project analyzes sentiment patterns in Reddit mental health communities (r/depression, r/mentalhealth, r/Anxiety) over a 5-year period (2020-2025) to understand how the COVID-19 pandemic affected mental health discourse online. The analysis uses VADER sentiment analysis and tracks specific COVID-related terminology to measure changes in community sentiment before, during, and after the pandemic.",
    "crumbs": [
      "View on GitHub",
      "How to Use"
    ]
  },
  {
    "objectID": "index.html#important-safety-and-ethics-disclaimer",
    "href": "index.html#important-safety-and-ethics-disclaimer",
    "title": "",
    "section": "Important Safety and Ethics Disclaimer",
    "text": "Important Safety and Ethics Disclaimer\n\nData Sensitivity Warning\nThis project analyzes data from mental health support communities where individuals share deeply personal experiences, including discussions of:\n\nDepression and anxiety\nSuicidal ideation\nSelf-harm\nTrauma and crisis situations\nMedical and therapeutic experiences\n\n⚠️ CRITICAL SAFETY CONSIDERATIONS:\n\nThis analysis is for research purposes only and should NOT be used to:\n\nDiagnose or assess individual mental health conditions\nReplace professional mental health services\nMake clinical decisions\nIdentify or target vulnerable individuals\n\nEthical Responsibilities:\n\nAll data analysis must respect user privacy and anonymity\nNo attempts should be made to identify individual users\nResults should be presented in aggregate form only\nFindings should not stigmatize mental health conditions\n\nContent Warning:\n\nThe raw data may contain triggering content\nResearchers should practice self-care and take breaks when needed\nIf you’re experiencing mental health challenges, please seek professional help\n\nLimitations:\n\nOnline posts may not accurately represent clinical mental health states\nSelf-selection bias affects who participates in these communities\nSentiment analysis tools have limitations with nuanced emotional expression\nResults cannot be generalized to all individuals with mental health conditions",
    "crumbs": [
      "View on GitHub",
      "How to Use"
    ]
  },
  {
    "objectID": "index.html#project-structure",
    "href": "index.html#project-structure",
    "title": "",
    "section": "Project Structure",
    "text": "Project Structure\n\nData Collection\n\ncollecting_over_5_years.ipynb: Collects Reddit posts from mental health subreddits using the PullPush API\n\nFetches up to 100 posts per month per subreddit\nCovers January 2020 to November 2025\nIncludes r/depression, r/mentalhealth, and r/Anxiety\n\n\n\n\nData Processing\n\ndata_cleaning.ipynb: Cleans and prepares the raw Reddit data\n\nHandles missing values and data formatting\nRemoves deleted content\nPrepares text for analysis\n\n\n\n\nAnalysis Notebooks\n\ndata_analysis.ipynb: Main sentiment analysis using VADER\n\nCalculates compound, positive, negative, and neutral sentiment scores\nGroups data by year and COVID period (Pre/During/Post)\nVisualizes sentiment trends over time\n\ncovid_specific_data.ipynb: Tracks COVID-related terminology\n\nMonitors pandemic-specific vocabulary\nAnalyzes therapy and mental health service terminology\nMeasures changes in isolation/lockdown language\n\ndata_word_frequency.ipynb: Word frequency analysis\n\nIdentifies common themes and topics\nTracks vocabulary changes over time",
    "crumbs": [
      "View on GitHub",
      "How to Use"
    ]
  },
  {
    "objectID": "index.html#key-findings-structure",
    "href": "index.html#key-findings-structure",
    "title": "",
    "section": "Key Findings Structure",
    "text": "Key Findings Structure\nThe analysis reveals sentiment patterns across three periods: - Pre-COVID (before March 2020) - During COVID (March 2020 - May 2023) - Post-COVID (after May 2023)",
    "crumbs": [
      "View on GitHub",
      "How to Use"
    ]
  },
  {
    "objectID": "index.html#dependencies",
    "href": "index.html#dependencies",
    "title": "",
    "section": "Dependencies",
    "text": "Dependencies\nasttokens                 3.0.0\nattrs                     25.4.0\nbeautifulsoup4            4.14.2\nbleach                    6.3.0\nblinker                   1.9.0\ncertifi                   2025.10.5\ncharset-normalizer        3.4.4\nclick                     8.3.0\ncomm                      0.2.3\ncontourpy                 1.3.3\ncycler                    0.12.1\ndash                      3.2.0\ndebugpy                   1.8.17\ndecorator                 5.2.1\ndefusedxml                0.7.1\nexecuting                 2.2.1\nfastjsonschema            2.21.2\nfilelock                  3.20.0\nFlask                     3.1.2\nfonttools                 4.60.1\nfsspec                    2025.10.0\nhf-xet                    1.2.0\nhuggingface-hub           0.36.0\nidna                      3.11\nimportlib_metadata        8.7.0\nipykernel                 7.1.0\nipython                   9.6.0\nipython_pygments_lexers   1.1.1\nitsdangerous              2.2.0\njedi                      0.19.2\nJinja2                    3.1.6\njoblib                    1.5.2\njsonschema                4.25.1\njsonschema-specifications 2025.9.1\njupyter_client            8.6.3\njupyter_core              5.9.1\njupyterlab_pygments       0.3.0\nkiwisolver                1.4.9\nMarkupSafe                3.0.3\nmatplotlib                3.10.7\nmatplotlib-inline         0.2.1\nmistune                   3.1.4\nmpmath                    1.3.0\nnarwhals                  2.10.0\nnbclient                  0.10.2\nnbconvert                 7.16.6\nnbformat                  5.10.4\nnest-asyncio              1.6.0\nnetworkx                  3.5\nnltk                      3.9.2\nnumpy                     2.3.4\nnvidia-cublas-cu12        12.8.4.1\nnvidia-cuda-cupti-cu12    12.8.90\nnvidia-cuda-nvrtc-cu12    12.8.93\nnvidia-cuda-runtime-cu12  12.8.90\nnvidia-cudnn-cu12         9.10.2.21\nnvidia-cufft-cu12         11.3.3.83\nnvidia-cufile-cu12        1.13.1.3\nnvidia-curand-cu12        10.3.9.90\nnvidia-cusolver-cu12      11.7.3.90\nnvidia-cusparse-cu12      12.5.8.93\nnvidia-cusparselt-cu12    0.7.1\nnvidia-nccl-cu12          2.27.5\nnvidia-nvjitlink-cu12     12.8.93\nnvidia-nvshmem-cu12       3.3.20\nnvidia-nvtx-cu12          12.8.90\npackaging                 25.0\npandas                    2.3.3\npandocfilters             1.5.1\nparso                     0.8.5\npexpect                   4.9.0\npillow                    12.0.0\npip                       25.3\nplatformdirs              4.5.0\nplotly                    6.3.1\npraw                      7.8.1\nprawcore                  2.4.0\nprompt_toolkit            3.0.52\npsutil                    7.1.2\nptyprocess                0.7.0\npure_eval                 0.2.3\nPygments                  2.19.2\npyparsing                 3.2.5\npython-dateutil           2.9.0\npytz                      2025.2\nPyYAML                    6.0.3\npyzmq                     27.1.0\nreferencing               0.37.0\nregex                     2025.10.23\nrequests                  2.32.5\nretrying                  1.4.2\nrpds-py                   0.28.0\nsafetensors               0.6.2\nscikit-learn              1.7.2\nscipy                     1.16.3\nsetuptools                80.9.0\nsix                       1.17.0\nsoupsieve                 2.8\nstack-data                0.6.3\nsympy                     1.14.0\nthreadpoolctl             3.6.0\ntinycss2                  1.4.0\ntokenizers                0.22.1\ntorch                     2.9.0\ntornado                   6.5.2\ntqdm                      4.67.1\ntraitlets                 5.14.3\ntransformers              4.57.1\ntriton                    3.5.0\ntyping_extensions         4.15.0\ntzdata                    2025.2\nupdate-checker            0.18.0\nurllib3                   2.5.0\nvaderSentiment            3.3.2\nwcwidth                   0.2.14\nwebencodings              0.5.1\nwebsocket-client          1.9.0\nWerkzeug                  3.1.3\nwordcloud                 1.9.4\nzipp                      3.23.0",
    "crumbs": [
      "View on GitHub",
      "How to Use"
    ]
  },
  {
    "objectID": "index.html#usage-guidelines",
    "href": "index.html#usage-guidelines",
    "title": "",
    "section": "Usage Guidelines",
    "text": "Usage Guidelines\n\nBefore Running:\n\nReview all safety disclaimers\nEnsure you have appropriate IRB approval if conducting academic research\nConsider the ethical implications of your analysis\n\nRunning the Analysis:\n\nExecute notebooks in order: collection → cleaning → analysis\nData collection respects API rate limits\nAnalysis focuses on aggregate patterns, not individual posts\n\nInterpreting Results:\n\nRemember correlation does not imply causation\nConsider multiple factors affecting mental health discourse\nAcknowledge limitations in methodology",
    "crumbs": [
      "View on GitHub",
      "How to Use"
    ]
  },
  {
    "objectID": "index.html#data-privacy-and-storage",
    "href": "index.html#data-privacy-and-storage",
    "title": "",
    "section": "Data Privacy and Storage",
    "text": "Data Privacy and Storage\n\nNo personally identifiable information should be stored\nFollow Reddit’s terms of service and API guidelines\nStore data securely and limit access\nDelete data when analysis is complete",
    "crumbs": [
      "View on GitHub",
      "How to Use"
    ]
  },
  {
    "objectID": "index.html#resources-for-mental-health-support",
    "href": "index.html#resources-for-mental-health-support",
    "title": "",
    "section": "Resources for Mental Health Support",
    "text": "Resources for Mental Health Support\nIf you or someone you know is struggling with mental health:\n\nNational Suicide Prevention Lifeline: 988 (US)\nCrisis Text Line: Text HOME to 741741\nInternational Crisis Lines: findahelpline.com\nReddit Crisis Resources: reddit.com/r/SuicideWatch/wiki/hotlines",
    "crumbs": [
      "View on GitHub",
      "How to Use"
    ]
  },
  {
    "objectID": "index.html#contributing-and-contact",
    "href": "index.html#contributing-and-contact",
    "title": "",
    "section": "Contributing and Contact",
    "text": "Contributing and Contact\nThis project is for research purposes. Any contributions should: - Maintain ethical standards - Respect the sensitivity of the data - Include appropriate disclaimers - Focus on aggregate insights\n\nRemember: Behind every data point is a real person seeking support. Treat this data with the respect and care it deserves.",
    "crumbs": [
      "View on GitHub",
      "How to Use"
    ]
  },
  {
    "objectID": "notebooks/topic_modeling.html#pre-covid-topics",
    "href": "notebooks/topic_modeling.html#pre-covid-topics",
    "title": "",
    "section": "Pre-COVID Topics",
    "text": "Pre-COVID Topics\n(n=576 posts) Interpretation: With only 576 posts, this is your smallest dataset. The topics show:\nTopic 1: Social isolation and loneliness (“alone”, “everyone”, “nye” suggesting New Year’s Eve loneliness)\nTopic 2: Mental health struggles, particularly anxiety related to school (“mental”, “anxiety”, “health”, “school”)\nTopic 3: General anxiety and emotional states (“anxiety”, “anxious”, “happy”, “tired”)\nTopic 4: Hope and improvement (“new”, “happy”, “better”, “hope”) mixed with anxiety\nTopic 5: Clinical anxiety symptoms (“panic”, “attack”, “sleep”, “depression”)\nKey insight: Even before COVID, anxiety and mental health were prominent topics, with distinct themes around panic attacks and social isolation.",
    "crumbs": [
      "View on GitHub",
      "Topic Modeling"
    ]
  },
  {
    "objectID": "notebooks/topic_modeling.html#during-covid-topics",
    "href": "notebooks/topic_modeling.html#during-covid-topics",
    "title": "",
    "section": "During COVID Topics",
    "text": "During COVID Topics\n(n=10,517 posts) The topics show:\nTopic 1: Physical anxiety symptoms (“panic”, “attack”, “sleep”, “heart”) - very clinical\nTopic 2: Life disruption (“job”, “mental health”, “home”, “house”) - likely reflecting lockdown impacts\nTopic 3: Deep emotional struggles (“depression”, “thoughts”, “mental”, “self”) - more serious mental health concerns\nTopic 4: Exhaustion and negativity (“anymore”, “hate”, “tired”, “never”, “fucking”)\nTopic 5: Youth and family dynamics (“school”, “friend”, “parents”, “mom”) - note “deleted user” suggests some users deleted their accounts\nKey insight: The dramatic increase in posts and the shift toward more serious mental health language (depression, suicidal ideation implied by “thoughts”) suggests COVID significantly intensified mental health struggles, particularly around isolation, disrupted routines, and family stress.",
    "crumbs": [
      "View on GitHub",
      "Topic Modeling"
    ]
  },
  {
    "objectID": "notebooks/topic_modeling.html#post-covid-topics",
    "href": "notebooks/topic_modeling.html#post-covid-topics",
    "title": "",
    "section": "Post-COVID Topics",
    "text": "Post-COVID Topics\nAbout 60% of the during-COVID volume, but still 10x higher than pre-COVID. Topics show:\nTopic 1: Communication and relationships (“talk”, “friend”, “say”, “told”)\nTopic 2: Life circumstances and family (“job”, “live”, “parents”, “school”)\nTopic 3: Depression and emotional fatigue (“depression”, “tired”, “thoughts”)\nTopic 4: Physical anxiety symptoms persist (“anxiety”, “heart”, “symptoms”, “pain”, “panic”)\nTopic 5: Continued panic disorder (“panic”, “anxious”, “attacks”)\nKey insight: While volume decreased from during-COVID, it remains much higher than pre-COVID levels, suggesting lasting mental health impacts. The persistence of clinical anxiety symptoms (Topics 4 & 5) indicates that anxiety disorders triggered or worsened during COVID haven’t fully resolved.",
    "crumbs": [
      "View on GitHub",
      "Topic Modeling"
    ]
  },
  {
    "objectID": "notebooks/data_word_frequency.html#pre-covid-word-cloud",
    "href": "notebooks/data_word_frequency.html#pre-covid-word-cloud",
    "title": "Identifying Shifts in Mental Health Discourse across COVID periods",
    "section": "Pre-COVID Word Cloud",
    "text": "Pre-COVID Word Cloud\nThe visualization shows:\n“feel” and “anxiety” are the dominant words (largest size)\n“friends”, “know”, and “people” are prominent, suggesting social connection themes\nMental health terms: “mental,” “depression,” “panic” are visible\nSocial contexts: “family,” “school,” “work,” “job”\nTemporal words: “never,” “always,” “still,” “anymore”\nThe pre-COVID discourse appears focused on emotional states (“feel,” “feeling”) and social relationships (“friends,” “people,” “family”), with anxiety as a central concern.\n\n\nCode\nplt.figure(figsize=(10, 5))\nplt.imshow(wordcloud_during_covid, interpolation=\"bilinear\")\nplt.title(\"World Cloud, During-COVID\")\nplt.axis(\"off\")\nplt.show()",
    "crumbs": [
      "View on GitHub",
      "Word Frequency"
    ]
  },
  {
    "objectID": "notebooks/data_word_frequency.html#during-covid-word-cloud",
    "href": "notebooks/data_word_frequency.html#during-covid-word-cloud",
    "title": "Identifying Shifts in Mental Health Discourse across COVID periods",
    "section": "During-COVID Word Cloud",
    "text": "During-COVID Word Cloud\nNotable changes:\n“know” becomes much more prominent, suggesting uncertainty and information-seeking\n“anyone” appears very large, indicating increased feelings of isolation and seeking validation\n“feel” and “anxiety” remain dominant but are now surrounded by:\nTime-related words: “day,” “time,” “still,” “never”\nCognitive words: “think,” “thought”\nExistence/routine words: “life,” “going,” “getting”\n“help” is very prominent, suggesting increased help-seeking behavior\n“everything” and “anything” appear larger, indicating all-encompassing feelings\nThe during-COVID discourse shows a shift toward uncertainty (“know”), universality of experience (“anyone,” “everyone”), and disruption of normal life patterns.\n\n\nCode\nplt.figure(figsize=(10, 5))\nplt.imshow(wordcloud_post_covid, interpolation=\"bilinear\")\nplt.title(\"World Cloud, Post-COVID\")\nplt.axis(\"off\")\nplt.show()",
    "crumbs": [
      "View on GitHub",
      "Word Frequency"
    ]
  },
  {
    "objectID": "notebooks/data_word_frequency.html#post-covid-word-cloud",
    "href": "notebooks/data_word_frequency.html#post-covid-word-cloud",
    "title": "Identifying Shifts in Mental Health Discourse across COVID periods",
    "section": "Post-COVID Word Cloud",
    "text": "Post-COVID Word Cloud\nThe post-COVID cloud shows:\n“feel” and “know” remain the largest words\n“anxiety” is still very prominent\n“feeling” appears large, suggesting continued emotional processing\nWords suggesting persistence:\n“still,” “always,” “never”\n“help” remains prominent but slightly smaller than during COVID\nTemporal words: “time,” “day,” “long” indicate ongoing struggles\n“trying” appears, suggesting active coping efforts\nSocial words remain: “people,” “friends,” “anyone”\nThe post-COVID discourse suggests people are still processing emotional fallout, with language indicating both ongoing struggles (“still,” “never”) and active coping (“trying,” “getting”).",
    "crumbs": [
      "View on GitHub",
      "Word Frequency"
    ]
  },
  {
    "objectID": "notebooks/data_word_frequency.html#comparing-the-three-word-clouds-reveals",
    "href": "notebooks/data_word_frequency.html#comparing-the-three-word-clouds-reveals",
    "title": "Identifying Shifts in Mental Health Discourse across COVID periods",
    "section": "Comparing the three word clouds reveals**:",
    "text": "Comparing the three word clouds reveals**:\n\nEmotional continuity\n“Feel” and “anxiety” dominate all three periods, confirming anxiety as a persistent mental health concern\n\n\nCOVID amplification\nThe During-COVID cloud is visually denser and more chaotic, reflecting the volume and intensity increase\n\n\nIsolation language\n“Anyone” and “everyone” become much more prominent during COVID, showing the pandemic’s social isolation impact\n\n\nCognitive shift\n“Know” and “think” grow in prominence during/after COVID, suggesting more uncertainty and cognitive processing\n\n\nPersistent impact\nThe Post-COVID cloud resembles During-COVID more than Pre-COVID, confirming that mental health discourse hasn’t returned to baseline\n\n\nHelp-seeking behavior\n“Help” is consistently prominent but peaks during COVID, indicating the pandemic drove people to actively seek support\nThe word frequency analysis complements your topic modeling by showing that while the core vocabulary remains similar, the intensity, context, and proportions shifted dramatically during COVID and haven’t fully recovered.",
    "crumbs": [
      "View on GitHub",
      "Word Frequency"
    ]
  },
  {
    "objectID": "notebooks/covid_specific_data.html#analyzing-specific-stressors-and-covid-related-vocab",
    "href": "notebooks/covid_specific_data.html#analyzing-specific-stressors-and-covid-related-vocab",
    "title": "Analyzing specific stressors and COVID-related vocab",
    "section": "",
    "text": "Code\ncovid_terms = {\n    'pandemic_terms': ['pandemic', 'covid', 'coronavirus', 'virus', 'outbreak'],\n    'lockdown_terms': ['lockdown', 'quarantine', 'isolation', 'isolating', 'isolated', 'alone'],\n    'remote_terms': ['remote', 'virtual', 'online', 'zoom', 'video', 'wfh', 'work from home'],\n    'health_safety': ['mask', 'vaccine', 'vaccinated', 'social distancing', 'distance'],\n    'therapy_terms': ['therapist', 'therapy', 'counseling', 'telehealth', 'virtual therapy', 'online therapy']\n}\n\n\n\n\nCode\ndef track_specific_terms(df, terms_dict, text_column=\"full_text\"):\n    results = {}\n    \n    for period in df['covid_period'].unique():\n        period_data = df[df['covid_period'] == period]\n        period_results = {}\n        \n        # get all text for i-th period\n        all_text = ' '.join(period_data[text_column].fillna('').str.lower())\n        total_words = len(all_text.split())\n        \n        for category, terms in terms_dict.items():\n            category_count = 0\n            term_counts = {}\n            \n            for term in terms:\n                count = all_text.count(term.lower())\n                term_counts[term] = count\n                category_count += count\n                \n            period_results[category] = {\n                \"total_count\": category_count,\n                \"frequency_per_1000\": (category_count / total_words * 1000) if total_words &gt; 0 else 0,\n                'term_breakdown': term_counts\n            }\n        \n        results[period] = period_results\n        \n    return results\n\n\n\n\nCode\n%store -r reddit_sent_df\n\n\n\n\nCode\ncovid_tracking = track_specific_terms(reddit_sent_df, covid_terms)\n\n\n\n\nCode\nprint(\"COVID-Related Term Frequencies by Period\")\nprint(\"=\" * 60)\n\nfor period in ['Pre-COVID', 'During COVID', 'Post-COVID']:\n    if period in covid_tracking:\n        print(f\"\\n{period}:\")\n        for category, data in covid_tracking[period].items():\n            print(f\"\\n  {category}:\")\n            print(f\"    Total mentions: {data['total_count']}\")\n            print(f\"    Per 1000 words: {data['frequency_per_1000']:.2f}\")\n            \n            # Show top terms in category\n            top_terms = sorted(data['term_breakdown'].items(), key=lambda x: x[1], reverse=True)[:3]\n            if any(count &gt; 0 for _, count in top_terms):\n                print(\"    Most mentioned:\")\n                for term, count in top_terms:\n                    if count &gt; 0:\n                        print(f\"      - {term}: {count}\")\n\n\nCOVID-Related Term Frequencies by Period\n============================================================\n\nPre-COVID:\n\n  pandemic_terms:\n    Total mentions: 3\n    Per 1000 words: 0.05\n    Most mentioned:\n      - virus: 2\n      - coronavirus: 1\n\n  lockdown_terms:\n    Total mentions: 91\n    Per 1000 words: 1.47\n    Most mentioned:\n      - alone: 81\n      - isolated: 8\n      - isolation: 1\n\n  remote_terms:\n    Total mentions: 26\n    Per 1000 words: 0.42\n    Most mentioned:\n      - online: 14\n      - video: 10\n      - remote: 1\n\n  health_safety:\n    Total mentions: 5\n    Per 1000 words: 0.08\n    Most mentioned:\n      - vaccine: 2\n      - distance: 2\n      - mask: 1\n\n  therapy_terms:\n    Total mentions: 69\n    Per 1000 words: 1.12\n    Most mentioned:\n      - therapist: 38\n      - therapy: 30\n      - counseling: 1\n\nDuring COVID:\n\n  pandemic_terms:\n    Total mentions: 661\n    Per 1000 words: 0.56\n    Most mentioned:\n      - covid: 378\n      - pandemic: 180\n      - virus: 71\n\n  lockdown_terms:\n    Total mentions: 1298\n    Per 1000 words: 1.09\n    Most mentioned:\n      - alone: 978\n      - quarantine: 88\n      - lockdown: 70\n\n  remote_terms:\n    Total mentions: 723\n    Per 1000 words: 0.61\n    Most mentioned:\n      - video: 300\n      - online: 297\n      - remote: 45\n\n  health_safety:\n    Total mentions: 200\n    Per 1000 words: 0.17\n    Most mentioned:\n      - mask: 82\n      - distance: 64\n      - vaccine: 37\n\n  therapy_terms:\n    Total mentions: 1523\n    Per 1000 words: 1.28\n    Most mentioned:\n      - therapy: 719\n      - therapist: 643\n      - counseling: 154\n\nPost-COVID:\n\n  pandemic_terms:\n    Total mentions: 147\n    Per 1000 words: 0.16\n    Most mentioned:\n      - covid: 97\n      - pandemic: 35\n      - virus: 13\n\n  lockdown_terms:\n    Total mentions: 897\n    Per 1000 words: 0.96\n    Most mentioned:\n      - alone: 762\n      - isolated: 62\n      - isolation: 33\n\n  remote_terms:\n    Total mentions: 453\n    Per 1000 words: 0.49\n    Most mentioned:\n      - video: 205\n      - online: 186\n      - remote: 26\n\n  health_safety:\n    Total mentions: 106\n    Per 1000 words: 0.11\n    Most mentioned:\n      - distance: 51\n      - mask: 50\n      - vaccine: 4\n\n  therapy_terms:\n    Total mentions: 1085\n    Per 1000 words: 1.16\n    Most mentioned:\n      - therapy: 626\n      - therapist: 421\n      - counseling: 28\n\n\n\n\nCode\n# Specific Stressor Analysis\nstressor_terms = {\n    \"job_related\": [\n        \"job\",\n        \"work\",\n        \"unemployed\",\n        \"fired\",\n        \"laid off\",\n        \"career\",\n        \"boss\",\n        \"workplace\",\n        \"employment\",\n    ],\n    \"social_isolation\": [\n        \"alone\",\n        \"lonely\",\n        \"isolated\",\n        \"friends\",\n        \"social\",\n        \"nobody\",\n        \"anyone\",\n        \"loneliness\",\n    ],\n    \"health_anxiety\": [\n        \"health\",\n        \"sick\",\n        \"illness\",\n        \"symptoms\",\n        \"doctor\",\n        \"medical\",\n        \"hospital\",\n        \"disease\",\n    ],\n    \"financial_stress\": [\n        \"money\",\n        \"bills\",\n        \"rent\",\n        \"financial\",\n        \"afford\",\n        \"broke\",\n        \"debt\",\n        \"pay\",\n    ],\n    \"family_issues\": [\n        \"family\",\n        \"parents\",\n        \"mother\",\n        \"father\",\n        \"mom\",\n        \"dad\",\n        \"siblings\",\n        \"relationship\",\n    ],\n    \"academic_stress\": [\n        \"school\",\n        \"college\",\n        \"university\",\n        \"exam\",\n        \"study\",\n        \"student\",\n        \"class\",\n        \"grade\",\n    ],\n    # Below are categories made from doing a machine learning clustering algorithm\n    # trying to find patterns in the text that I couldn't have come up with\n    \"health_anxiety\": [\n        \"heart\",\n        \"symptoms\",\n        \"panic attack\",\n        \"panic attacks\",\n        \"scared\",\n        \"pain\",\n        \"health\",\n        \"anxious\",\n        \"attack\",\n    ],\n    \"work_stress\": [\"job\", \"home\", \"house\", \"wfh\", \"remote\", \"work\"],\n    \"school_stress\": [\n        \"school\",\n        \"parents\",\n        \"mom\",\n        \"dad\",\n        \"remote school\",\n        \"class\",\n        \"online class\",\n    ],\n    \"burnout\": [\"tired\", \"anymore\", \"hate\", \"exhausted\", \"fucking tired\", \"end\"],\n}\n\n\n\n\nCode\nstressor_tracking = track_specific_terms(reddit_sent_df, stressor_terms)\n\n\n\n\nCode\nprint(\"\\n\\nStressor-Related Term Frequencies by Period\")\nprint(\"=\" * 60)\n\nfor period in ['Pre-COVID', 'During COVID', 'Post-COVID']:\n    if period in stressor_tracking:\n        print(f\"\\n{period}:\")\n        for category, data in stressor_tracking[period].items():\n            print(f\"\\n  {category}:\")\n            print(f\"    Total mentions: {data['total_count']}\")\n            print(f\"    Per 1000 words: {data['frequency_per_1000']:.2f}\")\n            \n            # Show top terms in category\n            top_terms = sorted(data['term_breakdown'].items(), key=lambda x: x[1], reverse=True)[:3]\n            if any(count &gt; 0 for _, count in top_terms):\n                print(\"    Most mentioned:\")\n                for term, count in top_terms:\n                    if count &gt; 0:\n                        print(f\"      - {term}: {count}\")\n\n\n\n\nStressor-Related Term Frequencies by Period\n============================================================\n\nPre-COVID:\n\n  job_related:\n    Total mentions: 264\n    Per 1000 words: 4.27\n    Most mentioned:\n      - work: 165\n      - job: 79\n      - career: 8\n\n  social_isolation:\n    Total mentions: 398\n    Per 1000 words: 6.43\n    Most mentioned:\n      - friends: 132\n      - anyone: 102\n      - alone: 81\n\n  health_anxiety:\n    Total mentions: 370\n    Per 1000 words: 5.98\n    Most mentioned:\n      - health: 78\n      - attack: 55\n      - scared: 49\n\n  financial_stress:\n    Total mentions: 185\n    Per 1000 words: 2.99\n    Most mentioned:\n      - rent: 96\n      - money: 30\n      - pay: 24\n\n  family_issues:\n    Total mentions: 352\n    Per 1000 words: 5.69\n    Most mentioned:\n      - family: 87\n      - mom: 78\n      - relationship: 47\n\n  academic_stress:\n    Total mentions: 174\n    Per 1000 words: 2.81\n    Most mentioned:\n      - school: 81\n      - class: 20\n      - college: 17\n\n  work_stress:\n    Total mentions: 354\n    Per 1000 words: 5.72\n    Most mentioned:\n      - work: 165\n      - job: 79\n      - home: 66\n\n  school_stress:\n    Total mentions: 253\n    Per 1000 words: 4.09\n    Most mentioned:\n      - school: 81\n      - mom: 78\n      - parents: 46\n\n  burnout:\n    Total mentions: 589\n    Per 1000 words: 9.52\n    Most mentioned:\n      - end: 415\n      - hate: 75\n      - anymore: 66\n\nDuring COVID:\n\n  job_related:\n    Total mentions: 4756\n    Per 1000 words: 4.01\n    Most mentioned:\n      - work: 3217\n      - job: 1195\n      - career: 132\n\n  social_isolation:\n    Total mentions: 6633\n    Per 1000 words: 5.59\n    Most mentioned:\n      - friends: 2158\n      - anyone: 2126\n      - alone: 978\n\n  health_anxiety:\n    Total mentions: 7629\n    Per 1000 words: 6.43\n    Most mentioned:\n      - health: 1515\n      - anxious: 1066\n      - attack: 1048\n\n  financial_stress:\n    Total mentions: 3707\n    Per 1000 words: 3.13\n    Most mentioned:\n      - rent: 2271\n      - money: 410\n      - broke: 379\n\n  family_issues:\n    Total mentions: 5533\n    Per 1000 words: 4.67\n    Most mentioned:\n      - mom: 1307\n      - family: 1262\n      - parents: 942\n\n  academic_stress:\n    Total mentions: 3588\n    Per 1000 words: 3.03\n    Most mentioned:\n      - school: 1456\n      - college: 517\n      - class: 467\n\n  work_stress:\n    Total mentions: 6149\n    Per 1000 words: 5.19\n    Most mentioned:\n      - work: 3217\n      - job: 1195\n      - home: 1059\n\n  school_stress:\n    Total mentions: 4738\n    Per 1000 words: 4.00\n    Most mentioned:\n      - school: 1456\n      - mom: 1307\n      - parents: 942\n\n  burnout:\n    Total mentions: 10167\n    Per 1000 words: 8.57\n    Most mentioned:\n      - end: 6449\n      - hate: 1485\n      - anymore: 1247\n\nPost-COVID:\n\n  job_related:\n    Total mentions: 3787\n    Per 1000 words: 4.06\n    Most mentioned:\n      - work: 2512\n      - job: 1017\n      - career: 97\n\n  social_isolation:\n    Total mentions: 4932\n    Per 1000 words: 5.29\n    Most mentioned:\n      - anyone: 1655\n      - friends: 1465\n      - alone: 762\n\n  health_anxiety:\n    Total mentions: 6794\n    Per 1000 words: 7.29\n    Most mentioned:\n      - health: 1197\n      - attack: 956\n      - scared: 810\n\n  financial_stress:\n    Total mentions: 2751\n    Per 1000 words: 2.95\n    Most mentioned:\n      - rent: 1615\n      - money: 311\n      - broke: 292\n\n  family_issues:\n    Total mentions: 4386\n    Per 1000 words: 4.70\n    Most mentioned:\n      - family: 1066\n      - mom: 1008\n      - parents: 644\n\n  academic_stress:\n    Total mentions: 2547\n    Per 1000 words: 2.73\n    Most mentioned:\n      - school: 988\n      - college: 405\n      - class: 335\n\n  work_stress:\n    Total mentions: 4892\n    Per 1000 words: 5.25\n    Most mentioned:\n      - work: 2512\n      - job: 1017\n      - home: 835\n\n  school_stress:\n    Total mentions: 3450\n    Per 1000 words: 3.70\n    Most mentioned:\n      - mom: 1008\n      - school: 988\n      - parents: 644\n\n  burnout:\n    Total mentions: 8314\n    Per 1000 words: 8.92\n    Most mentioned:\n      - end: 5314\n      - hate: 1164\n      - anymore: 989",
    "crumbs": [
      "View on GitHub",
      "COVID Stressor Analysis"
    ]
  },
  {
    "objectID": "notebooks/comparing_subreddits.html#pre-covid",
    "href": "notebooks/comparing_subreddits.html#pre-covid",
    "title": "",
    "section": "Pre-COVID",
    "text": "Pre-COVID\n\nr/Anxiety: ~-0.14 (least negative)\nr/depression: ~-0.18\nr/mentalhealth: ~-0.11 (least negative overall)\n\n\nInterpretation\nAll three subreddits show negative sentiment scores even before COVID, which is expected given they’re mental health support communities. r/mentalhealth is slightly less negative, possibly because it’s more focused on general wellness and recovery rather than specific disorders.",
    "crumbs": [
      "View on GitHub",
      "Comparing Subreddits"
    ]
  },
  {
    "objectID": "notebooks/comparing_subreddits.html#during-covid",
    "href": "notebooks/comparing_subreddits.html#during-covid",
    "title": "",
    "section": "During COVID",
    "text": "During COVID\n\nr/Anxiety: ~-0.23 (worsened significantly)\nr/depression: ~-0.21 (worsened)\nr/mentalhealth: ~-0.14 (worsened but remained least negative)\n\n\nInterpretation\nAll subreddits saw sentiment decline during COVID, with r/Anxiety experiencing the largest drop (from -0.14 to -0.23, a 64% increase in negativity). This aligns with research showing COVID-19 disproportionately impacted people with anxiety disorders due to uncertainty, isolation, and health fears. r/depression also worsened but less dramatically, while r/mentalhealth maintained its position as the least negative space.",
    "crumbs": [
      "View on GitHub",
      "Comparing Subreddits"
    ]
  },
  {
    "objectID": "notebooks/comparing_subreddits.html#post-covid",
    "href": "notebooks/comparing_subreddits.html#post-covid",
    "title": "",
    "section": "Post-COVID",
    "text": "Post-COVID\n\nr/Anxiety: ~-0.30 (continued worsening!)\nr/depression: ~-0.28 (continued worsening!)\nr/mentalhealth: ~-0.16 (countinued worsening!)\n\n\nInterpretation\nThis is the most alarming finding: sentiment continued to deteriorate post-COVID rather than recovering. r/Anxiety hit its lowest point at -0.30, representing a 114% increase in negativity from pre-COVID. r/depression also reached its nadir. This suggests the mental health crisis intensified after the acute pandemic phase, possibly due to accumulated trauma, ongoing disruption, or delayed mental health consequences.\nKeywords by Subreddit Analysis\n\n\nCode\nstressor_terms = {\n    'health_anxiety': ['heart', 'symptoms', 'panic attack', 'panic attacks', 'scared', 'pain', 'health', 'anxious', 'attack'],\n    'work_stress': ['job', 'home', 'house', 'wfh', 'remote', 'work'],\n    'school_stress': ['school', 'parents', 'mom', 'dad', 'remote school', 'class', 'online class'],\n    'burnout': ['tired', 'anymore', 'hate', 'exhausted', 'fucking tired', 'end'],\n    'therapy': ['therapist', 'therapy', 'counseling', 'telehealth', 'find help']\n}\nperiods = ['Pre-COVID', 'During COVID', 'Post-COVID']\n\nsubreddits = reddit_sent_df['subreddit'].unique()\ncategories = list(stressor_terms.keys())\nresults = []\n\n\nDefines five stressor categories with associated keywords, similar to the previous notebook but focused on comparing how these manifest across different subreddits. This will allow analysis of whether certain communities discuss specific stressors more than others.\n\n\nCode\ndef count_total_words(text_series):\n    if text_series.empty:\n        return 0\n    total_words = text_series.astype(str).str.split().str.len().sum()\n    return total_words\n\n\n# This function counts occurrences of a list of keywords\ndef count_keyword_mentions(text_series, keywords):\n    if text_series.empty:\n        return 0\n    # Create a regex pattern: 'word1|word2|word3'\n    pattern = r\"\\b(\" + \"|\".join(re.escape(k) for k in keywords) + r\")\\b\"\n    mentions = text_series.astype(str).str.count(pattern, flags=re.IGNORECASE).sum()\n    return int(mentions)\n\n\nExplanation\ncount_total_words():\nCounts total words in a series of texts by splitting on whitespace\ncount_keyword_mentions():\nUses regex to count keyword occurrences with word boundaries ( to avoid partial matches. The re.IGNORECASE flag ensures case-insensitive matching.\n\n\nCode\nfor period in periods:\n    for sub in subreddits:\n        # Create the subset of data\n        subset_df = reddit_sent_df[\n            (reddit_sent_df[\"covid_period\"] == period)\n            & (reddit_sent_df[\"subreddit\"] == sub)\n        ]\n\n        if subset_df.empty:\n            continue\n\n        # Get all text and total words for this subset\n        text_data = subset_df[\"full_text\"]\n        total_words = count_total_words(text_data)\n\n        if total_words == 0:\n            continue\n\n        # Calculate frequency for each category\n        for category, keywords in stressor_terms.items():\n            mentions = count_keyword_mentions(text_data, keywords)\n            # Calculate frequency per 1000 words\n            freq_per_1000 = (mentions / total_words) * 1000 if total_words &gt; 0 else 0\n\n            # Store the result\n            results.append(\n                {\n                    \"covid_period\": period,\n                    \"subreddit\": sub,\n                    \"category\": category,\n                    \"frequency_per_1000\": freq_per_1000,\n                    \"total_mentions\": mentions,\n                    \"total_words\": total_words,\n                }\n            )\n\n\nExplanation\nThis triple-nested loop iterates through each combination of period, subreddit, and stressor category. For each combination, it:\n\nFilters the data to that specific period and subreddit\nCounts total words in that subset\nCounts mentions of each stressor category’s keywords\nCalculates normalized frequency (per 1000 words) for fair comparison\nStores all results in a list, which is then converted to a DataFrame\n\nThis creates a comprehensive dataset showing how often each stressor is mentioned in each subreddit during each period.\n\n\nCode\nkeyword_freq_df = pd.DataFrame(results)\n\n\n\n\nCode\nsubreddit_order = sorted(keyword_freq_df['subreddit'].unique())\ncovid_period_order = [\"Pre-COVID\", \"During COVID\", \"Post-COVID\"]\n\nfig = px.bar(\n    keyword_freq_df,\n    x='subreddit',\n    y='frequency_per_1000',\n    color='covid_period',\n    facet_col='category',\n    facet_col_wrap=3,\n    barmode='group',\n    \n    category_orders={\n        'subreddit': subreddit_order,\n        'covid_period': covid_period_order\n    },\n    \n    title='Keyword Frequency by Subreddit, Period, and Category',\n    labels={\n        'subreddit': \"Subreddit\",\n        \"frequency_per_1000\": \"Frequency Per 1000 Words\",\n        'covid_period': \"COVID Period\"\n    },\n    \n    color_discrete_sequence=px.colors.sequential.Darkmint_r,\n    height=800,\n    \n    facet_row_spacing=0.15,\n    facet_col_spacing=0.05\n)\n\nfig.update_xaxes(tickangle=45, matches=None, showticklabels=True)\nfig.update_yaxes(matches=None, showticklabels=True)\nfig.for_each_annotation(lambda a: a.update(text=a.text.split(\"=\")[-1]))\nfig.update_layout(margin=dict(b=100))\n\nfig.show()\n\n\n                                                    \n\n\nExplanation\nCreates a complex faceted bar chart with:\nFacets: Separate panels for each stressor category (5 panels total)\nX-axis: Subreddits (Anxiety, depression, mentalhealth)\nY-axis: Frequency per 1000 words\nColor: COVID period (three shades showing temporal progression)\nLayout: 3 columns per row, 800px height\nUpdates axes to allow independent scales per facet and rotates x-axis labels 45 degrees for readability",
    "crumbs": [
      "View on GitHub",
      "Comparing Subreddits"
    ]
  },
  {
    "objectID": "notebooks/comparing_subreddits.html#panel-1-health-anxiety",
    "href": "notebooks/comparing_subreddits.html#panel-1-health-anxiety",
    "title": "",
    "section": "Panel 1: Health Anxiety",
    "text": "Panel 1: Health Anxiety\nr/depression: - Pre-COVID: ~2.0 - During COVID: ~2.3 - Post-COVID: ~2.2\nr/mentalhealth: - Pre-COVID: ~4.8 - During COVID: ~3.5 - Post-COVID: ~3.6\nr/Anxiety: - Pre-COVID: ~7.0 - During COVID: ~9.2 - Post-COVID: ~10.0\nInterpretation: r/Anxiety dominates health anxiety language, which makes sense given the subreddit’s focus. Strikingly, r/Anxiety’s health anxiety mentions increased 43% from pre-COVID to post-COVID (7.0 → 10.0 per 1000 words), showing sustained physical symptom focus even after the pandemic. r/mentalhealth actually decreased during COVID, possibly because general wellness discussions were crowded out by more acute concerns. r/depression remained relatively stable, suggesting depression discussions focus less on physical anxiety symptoms.",
    "crumbs": [
      "View on GitHub",
      "Comparing Subreddits"
    ]
  },
  {
    "objectID": "notebooks/comparing_subreddits.html#panel-2-work-stress",
    "href": "notebooks/comparing_subreddits.html#panel-2-work-stress",
    "title": "",
    "section": "Panel 2: Work Stress",
    "text": "Panel 2: Work Stress\nr/depression: - All periods: ~3.2-3.7\nr/mentalhealth: - Pre-COVID: ~3.0 - During COVID: ~3.5 - Post-COVID: ~3.2\nr/Anxiety: - Pre-COVID: ~3.0 - During COVID: ~5.8 - Post-COVID: ~4.2\nInterpretation: r/Anxiety showed dramatic work stress increase during COVID (nearly doubling from 3.0 to 5.8), reflecting how work-from-home, job insecurity, and workplace changes particularly triggered anxiety. The post-COVID decline to 4.2 suggests partial adaptation but still 40% above baseline. r/depression and r/mentalhealth remained relatively stable, indicating work stress isn’t as central to depression/general mental health discussions.",
    "crumbs": [
      "View on GitHub",
      "Comparing Subreddits"
    ]
  },
  {
    "objectID": "notebooks/comparing_subreddits.html#panel-3-school-stress",
    "href": "notebooks/comparing_subreddits.html#panel-3-school-stress",
    "title": "",
    "section": "Panel 3: School Stress",
    "text": "Panel 3: School Stress\nr/depression: - All periods: ~3.8-4.0\nr/mentalhealth: - Pre-COVID: ~3.5 - During COVID: ~3.4 - Post-COVID: ~3.3\nr/Anxiety: - Pre-COVID: ~3.4 - During COVID: ~3.6 - Post-COVID: ~3.4\nInterpretation: School stress showed remarkable stability across all subreddits and periods, hovering around 3.3-4.0 per 1000 words. This suggests academic stress is a constant background factor in these communities, relatively unaffected by COVID. The slight elevation in r/depression might reflect the higher prevalence of depression among students dealing with academic pressures.",
    "crumbs": [
      "View on GitHub",
      "Comparing Subreddits"
    ]
  },
  {
    "objectID": "notebooks/comparing_subreddits.html#panel-4-burnout",
    "href": "notebooks/comparing_subreddits.html#panel-4-burnout",
    "title": "",
    "section": "Panel 4: Burnout",
    "text": "Panel 4: Burnout\nr/Anxiety: - Pre-COVID: ~2.2 - During COVID: ~2.2 - Post-COVID: ~2.2\nr/depression: - Pre-COVID: ~4.7 - During COVID: ~5.0 - Post-COVID: ~7.6\nr/mentalhealth: - Pre-COVID: ~2.9 - During COVID: ~3.0 - Post-COVID: ~3.0\nInterpretation: This is the most striking panel. r/depression’s burnout language exploded post-COVID, increasing 62% from pre-COVID (4.7 → 7.6 per 1000 words). During COVID it was only slightly elevated (5.0), but post-COVID saw massive increase in words like “tired,” “anymore,” “hate,” and “end.” This suggests depression communities are experiencing severe exhaustion and possibly increased suicidal ideation in the aftermath of COVID.\nParadoxically, r/Anxiety’s burnout remained completely flat (~2.2 across all periods), suggesting anxiety manifests more as acute distress rather than chronic exhaustion. r/mentalhealth also remained stable, possibly because it’s a more solutions-focused community.",
    "crumbs": [
      "View on GitHub",
      "Comparing Subreddits"
    ]
  },
  {
    "objectID": "notebooks/comparing_subreddits.html#panel-5-therapy",
    "href": "notebooks/comparing_subreddits.html#panel-5-therapy",
    "title": "",
    "section": "Panel 5: Therapy",
    "text": "Panel 5: Therapy\nr/Anxiety: - Pre-COVID: ~0.95 - During COVID: ~0.7 - Post-COVID: ~1.0\nr/depression: - Pre-COVID: ~0.7 - During COVID: ~0.9 - Post-COVID: ~0.9\nr/mentalhealth: - Pre-COVID: ~1.2 - During COVID: ~1.4 - Post-COVID: ~1.35\nInterpretation: r/mentalhealth consistently discusses therapy most (1.2-1.4 per 1000 words), reinforcing its role as a resource-oriented community. The slight increase during COVID likely reflects telehealth discussions.\nr/Anxiety shows a U-shaped pattern: therapy mentions dropped during COVID (0.95 → 0.7), possibly because acute crisis posts crowded out treatment discussions, then rebounded post-COVID to baseline levels.\nr/depression showed modest increases, suggesting growing treatment-seeking behavior. Overall, therapy language remains relatively low across all communities (under 1.5 per 1000 words), which might indicate barriers to treatment access or stigma.",
    "crumbs": [
      "View on GitHub",
      "Comparing Subreddits"
    ]
  },
  {
    "objectID": "notebooks/comparing_subreddits.html#subreddit-specialization-is-real",
    "href": "notebooks/comparing_subreddits.html#subreddit-specialization-is-real",
    "title": "",
    "section": "1. Subreddit Specialization is Real:",
    "text": "1. Subreddit Specialization is Real:\n\nr/Anxiety: Highest in health anxiety (10.0) and work stress (5.8 during COVID)\nr/depression: Highest in burnout (7.6 post-COVID) and school stress (4.0)\nr/mentalhealth: Highest in therapy discussions (1.4) and most positive sentiment\n\nEach community has distinct concerns, validating their separate existence and suggesting tailored interventions would be more effective than one-size-fits-all approaches.",
    "crumbs": [
      "View on GitHub",
      "Comparing Subreddits"
    ]
  },
  {
    "objectID": "notebooks/comparing_subreddits.html#anxiety-disorders-react-more-acutely-to-covid",
    "href": "notebooks/comparing_subreddits.html#anxiety-disorders-react-more-acutely-to-covid",
    "title": "",
    "section": "2. Anxiety Disorders React More Acutely to COVID:",
    "text": "2. Anxiety Disorders React More Acutely to COVID:\nr/Anxiety showed the sharpest changes during COVID across multiple categories (work stress doubled, health anxiety spiked), while r/depression remained more stable during the acute phase but worsened dramatically afterward. This suggests:\n\nAnxiety disorders: Reactive to immediate stressors\nDepressive disorders: Delayed response, accumulated impact",
    "crumbs": [
      "View on GitHub",
      "Comparing Subreddits"
    ]
  },
  {
    "objectID": "notebooks/comparing_subreddits.html#post-covid-mental-health-crisis-is-concentrated-in-depression",
    "href": "notebooks/comparing_subreddits.html#post-covid-mental-health-crisis-is-concentrated-in-depression",
    "title": "",
    "section": "3. Post-COVID Mental Health Crisis is Concentrated in Depression:",
    "text": "3. Post-COVID Mental Health Crisis is Concentrated in Depression:\nThe sentiment chart shows both communities worsened post-COVID, but the burnout data reveals r/depression experienced particularly severe deterioration. The 62% increase in burnout language suggests:\n\nAccumulated fatigue from COVID stressors\nPossible economic hardship consequences\nLoss of social support structures\nIncreased hopelessness/suicidal ideation (high “end” frequency)",
    "crumbs": [
      "View on GitHub",
      "Comparing Subreddits"
    ]
  },
  {
    "objectID": "notebooks/comparing_subreddits.html#rmentalhealth-as-a-resilience-factor",
    "href": "notebooks/comparing_subreddits.html#rmentalhealth-as-a-resilience-factor",
    "title": "",
    "section": "4. r/mentalhealth as a Resilience Factor:",
    "text": "4. r/mentalhealth as a Resilience Factor:\nThis community consistently shows:\n\nLess negative sentiment than disorder-specific subreddits\nStable stressor levels across periods\nHighest therapy engagement\nFocus on solutions rather than symptoms\n\nThis suggests general mental health communities may provide more balanced support than diagnosis-specific spaces, though both serve important roles.",
    "crumbs": [
      "View on GitHub",
      "Comparing Subreddits"
    ]
  },
  {
    "objectID": "notebooks/comparing_subreddits.html#the-anxiety-to-depression-pipeline",
    "href": "notebooks/comparing_subreddits.html#the-anxiety-to-depression-pipeline",
    "title": "",
    "section": "5. The Anxiety-to-Depression Pipeline:",
    "text": "5. The Anxiety-to-Depression Pipeline:\nThe progression from acute anxiety during COVID to severe burnout/depression post-COVID across the overall dataset might represent a temporal mental health cascade:\n\nAcute COVID stress → increased anxiety (health fears, uncertainty)\nProlonged stress → exhaustion, decreased coping\nPost-COVID reality → burnout, depression, hopelessness\n\nThis aligns with research showing chronic anxiety can lead to depression when stressors persist without resolution.",
    "crumbs": [
      "View on GitHub",
      "Comparing Subreddits"
    ]
  },
  {
    "objectID": "paper.html",
    "href": "paper.html",
    "title": "",
    "section": "",
    "text": "Code\n\n\n\n\n\nPaper Analysis\nHere is the full analysis paper for our project.",
    "crumbs": [
      "View on GitHub",
      "Paper Analysis"
    ]
  },
  {
    "objectID": "overview.html",
    "href": "overview.html",
    "title": "",
    "section": "",
    "text": "Code",
    "crumbs": [
      "View on GitHub",
      "Project Overview"
    ]
  },
  {
    "objectID": "overview.html#project-overview",
    "href": "overview.html#project-overview",
    "title": "",
    "section": "Project Overview",
    "text": "Project Overview\nThis project analyzes sentiment patterns in Reddit mental health communities (r/depression, r/mentalhealth, r/Anxiety) over a 5-year period (2020-2025) to understand how the COVID-19 pandemic affected mental health discourse online. The analysis uses VADER sentiment analysis and tracks specific COVID-related terminology to measure changes in community sentiment before, during, and after the pandemic.",
    "crumbs": [
      "View on GitHub",
      "Project Overview"
    ]
  },
  {
    "objectID": "overview.html#important-safety-and-ethics-disclaimer",
    "href": "overview.html#important-safety-and-ethics-disclaimer",
    "title": "",
    "section": "Important Safety and Ethics Disclaimer",
    "text": "Important Safety and Ethics Disclaimer\n\nData Sensitivity Warning\nThis project analyzes data from mental health support communities where individuals share deeply personal experiences, including discussions of:\n\nDepression and anxiety\nSuicidal ideation\nSelf-harm\nTrauma and crisis situations\nMedical and therapeutic experiences\n\n⚠️ CRITICAL SAFETY CONSIDERATIONS:\n\nThis analysis is for research purposes only and should NOT be used to:\n\nDiagnose or assess individual mental health conditions\nReplace professional mental health services\nMake clinical decisions\nIdentify or target vulnerable individuals\n\nEthical Responsibilities:\n\nAll data analysis must respect user privacy and anonymity\nNo attempts should be made to identify individual users\nResults should be presented in aggregate form only\nFindings should not stigmatize mental health conditions\n\nContent Warning:\n\nThe raw data may contain triggering content\nResearchers should practice self-care and take breaks when needed\nIf you’re experiencing mental health challenges, please seek professional help\n\nLimitations:\n\nOnline posts may not accurately represent clinical mental health states\nSelf-selection bias affects who participates in these communities\nSentiment analysis tools have limitations with nuanced emotional expression\nResults cannot be generalized to all individuals with mental health conditions",
    "crumbs": [
      "View on GitHub",
      "Project Overview"
    ]
  },
  {
    "objectID": "overview.html#project-structure",
    "href": "overview.html#project-structure",
    "title": "",
    "section": "Project Structure",
    "text": "Project Structure\n\nData Collection\n\ncollecting_over_5_years.ipynb: Collects Reddit posts from mental health subreddits using the PullPush API\n\nFetches up to 100 posts per month per subreddit\nCovers January 2020 to November 2025\nIncludes r/depression, r/mentalhealth, and r/Anxiety\n\n\n\n\nData Processing\n\ndata_cleaning.ipynb: Cleans and prepares the raw Reddit data\n\nHandles missing values and data formatting\nRemoves deleted content\nPrepares text for analysis\n\n\n\n\nAnalysis Notebooks\n\ndata_analysis.ipynb: Main sentiment analysis using VADER\n\nCalculates compound, positive, negative, and neutral sentiment scores\nGroups data by year and COVID period (Pre/During/Post)\nVisualizes sentiment trends over time\n\ncovid_specific_data.ipynb: Tracks COVID-related terminology\n\nMonitors pandemic-specific vocabulary\nAnalyzes therapy and mental health service terminology\nMeasures changes in isolation/lockdown language\n\ndata_word_frequency.ipynb: Word frequency analysis\n\nIdentifies common themes and topics\nTracks vocabulary changes over time",
    "crumbs": [
      "View on GitHub",
      "Project Overview"
    ]
  },
  {
    "objectID": "overview.html#key-findings-structure",
    "href": "overview.html#key-findings-structure",
    "title": "",
    "section": "Key Findings Structure",
    "text": "Key Findings Structure\nThe analysis reveals sentiment patterns across three periods: - Pre-COVID (before March 2020) - During COVID (March 2020 - May 2023) - Post-COVID (after May 2023)",
    "crumbs": [
      "View on GitHub",
      "Project Overview"
    ]
  },
  {
    "objectID": "overview.html#dependencies",
    "href": "overview.html#dependencies",
    "title": "",
    "section": "Dependencies",
    "text": "Dependencies\nasttokens                 3.0.0\nattrs                     25.4.0\nbeautifulsoup4            4.14.2\nbleach                    6.3.0\nblinker                   1.9.0\ncertifi                   2025.10.5\ncharset-normalizer        3.4.4\nclick                     8.3.0\ncomm                      0.2.3\ncontourpy                 1.3.3\ncycler                    0.12.1\ndash                      3.2.0\ndebugpy                   1.8.17\ndecorator                 5.2.1\ndefusedxml                0.7.1\nexecuting                 2.2.1\nfastjsonschema            2.21.2\nfilelock                  3.20.0\nFlask                     3.1.2\nfonttools                 4.60.1\nfsspec                    2025.10.0\nhf-xet                    1.2.0\nhuggingface-hub           0.36.0\nidna                      3.11\nimportlib_metadata        8.7.0\nipykernel                 7.1.0\nipython                   9.6.0\nipython_pygments_lexers   1.1.1\nitsdangerous              2.2.0\njedi                      0.19.2\nJinja2                    3.1.6\njoblib                    1.5.2\njsonschema                4.25.1\njsonschema-specifications 2025.9.1\njupyter_client            8.6.3\njupyter_core              5.9.1\njupyterlab_pygments       0.3.0\nkiwisolver                1.4.9\nMarkupSafe                3.0.3\nmatplotlib                3.10.7\nmatplotlib-inline         0.2.1\nmistune                   3.1.4\nmpmath                    1.3.0\nnarwhals                  2.10.0\nnbclient                  0.10.2\nnbconvert                 7.16.6\nnbformat                  5.10.4\nnest-asyncio              1.6.0\nnetworkx                  3.5\nnltk                      3.9.2\nnumpy                     2.3.4\nnvidia-cublas-cu12        12.8.4.1\nnvidia-cuda-cupti-cu12    12.8.90\nnvidia-cuda-nvrtc-cu12    12.8.93\nnvidia-cuda-runtime-cu12  12.8.90\nnvidia-cudnn-cu12         9.10.2.21\nnvidia-cufft-cu12         11.3.3.83\nnvidia-cufile-cu12        1.13.1.3\nnvidia-curand-cu12        10.3.9.90\nnvidia-cusolver-cu12      11.7.3.90\nnvidia-cusparse-cu12      12.5.8.93\nnvidia-cusparselt-cu12    0.7.1\nnvidia-nccl-cu12          2.27.5\nnvidia-nvjitlink-cu12     12.8.93\nnvidia-nvshmem-cu12       3.3.20\nnvidia-nvtx-cu12          12.8.90\npackaging                 25.0\npandas                    2.3.3\npandocfilters             1.5.1\nparso                     0.8.5\npexpect                   4.9.0\npillow                    12.0.0\npip                       25.3\nplatformdirs              4.5.0\nplotly                    6.3.1\npraw                      7.8.1\nprawcore                  2.4.0\nprompt_toolkit            3.0.52\npsutil                    7.1.2\nptyprocess                0.7.0\npure_eval                 0.2.3\nPygments                  2.19.2\npyparsing                 3.2.5\npython-dateutil           2.9.0\npytz                      2025.2\nPyYAML                    6.0.3\npyzmq                     27.1.0\nreferencing               0.37.0\nregex                     2025.10.23\nrequests                  2.32.5\nretrying                  1.4.2\nrpds-py                   0.28.0\nsafetensors               0.6.2\nscikit-learn              1.7.2\nscipy                     1.16.3\nsetuptools                80.9.0\nsix                       1.17.0\nsoupsieve                 2.8\nstack-data                0.6.3\nsympy                     1.14.0\nthreadpoolctl             3.6.0\ntinycss2                  1.4.0\ntokenizers                0.22.1\ntorch                     2.9.0\ntornado                   6.5.2\ntqdm                      4.67.1\ntraitlets                 5.14.3\ntransformers              4.57.1\ntriton                    3.5.0\ntyping_extensions         4.15.0\ntzdata                    2025.2\nupdate-checker            0.18.0\nurllib3                   2.5.0\nvaderSentiment            3.3.2\nwcwidth                   0.2.14\nwebencodings              0.5.1\nwebsocket-client          1.9.0\nWerkzeug                  3.1.3\nwordcloud                 1.9.4\nzipp                      3.23.0",
    "crumbs": [
      "View on GitHub",
      "Project Overview"
    ]
  },
  {
    "objectID": "overview.html#usage-guidelines",
    "href": "overview.html#usage-guidelines",
    "title": "",
    "section": "Usage Guidelines",
    "text": "Usage Guidelines\n\nBefore Running:\n\nReview all safety disclaimers\nEnsure you have appropriate IRB approval if conducting academic research\nConsider the ethical implications of your analysis\n\nRunning the Analysis:\n\nExecute notebooks in order: collection → cleaning → analysis\nData collection respects API rate limits\nAnalysis focuses on aggregate patterns, not individual posts\n\nInterpreting Results:\n\nRemember correlation does not imply causation\nConsider multiple factors affecting mental health discourse\nAcknowledge limitations in methodology",
    "crumbs": [
      "View on GitHub",
      "Project Overview"
    ]
  },
  {
    "objectID": "overview.html#data-privacy-and-storage",
    "href": "overview.html#data-privacy-and-storage",
    "title": "",
    "section": "Data Privacy and Storage",
    "text": "Data Privacy and Storage\n\nNo personally identifiable information should be stored\nFollow Reddit’s terms of service and API guidelines\nStore data securely and limit access\nDelete data when analysis is complete",
    "crumbs": [
      "View on GitHub",
      "Project Overview"
    ]
  },
  {
    "objectID": "overview.html#resources-for-mental-health-support",
    "href": "overview.html#resources-for-mental-health-support",
    "title": "",
    "section": "Resources for Mental Health Support",
    "text": "Resources for Mental Health Support\nIf you or someone you know is struggling with mental health:\n\nNational Suicide Prevention Lifeline: 988 (US)\nCrisis Text Line: Text HOME to 741741\nInternational Crisis Lines: findahelpline.com\nReddit Crisis Resources: reddit.com/r/SuicideWatch/wiki/hotlines",
    "crumbs": [
      "View on GitHub",
      "Project Overview"
    ]
  },
  {
    "objectID": "overview.html#contributing-and-contact",
    "href": "overview.html#contributing-and-contact",
    "title": "",
    "section": "Contributing and Contact",
    "text": "Contributing and Contact\nThis project is for research purposes. Any contributions should: - Maintain ethical standards - Respect the sensitivity of the data - Include appropriate disclaimers - Focus on aggregate insights\n\nRemember: Behind every data point is a real person seeking support. Treat this data with the respect and care it deserves.",
    "crumbs": [
      "View on GitHub",
      "Project Overview"
    ]
  },
  {
    "objectID": "notebooks/raw_data.html",
    "href": "notebooks/raw_data.html",
    "title": "",
    "section": "",
    "text": "CodeShow All CodeHide All Code\n\n\n\n\n\n\nCode\nimport pandas as pd\nimport plotly.graph_objects as go\nimport plotly.io as pio\n\ndf = pd.read_csv(\"../reddit_data.csv\")\n\n\ndf_sample = df.head(200)\n\n\n\n\nCode\npio.renderers.default = \"notebook\"\nfig = go.Figure(data=[go.Table(\n    #--- Header ---\n    header=dict(values=list(df_sample.columns), # Get column names\n                fill_color='paleturquoise',\n                align='left'),\n    \n    #--- Cells ---\n    cells=dict(values=[df_sample[col] for col in df_sample.columns], # Get data\n               fill_color='lavender',\n               align='left'))\n])\n\nfig.update_layout(\n    title=\"Raw Reddit Data (First 200 Rows)\"\n)\n\nfig.show()",
    "crumbs": [
      "View on GitHub",
      "Raw Data"
    ]
  },
  {
    "objectID": "raw_data.html",
    "href": "raw_data.html",
    "title": "",
    "section": "",
    "text": "CodeShow All CodeHide All Code\n\n\n\n\nRaw Data\n\n\nCode\nimport pandas as pd\nimport plotly.graph_objects as go\nimport plotly.io as pio\n\ndf = pd.read_csv(\"reddit_data.csv\")\n\n\ndf_sample = df.head(200)\n\n\n\n\nCode\npio.renderers.default = \"notebook\"\nfig = go.Figure(data=[go.Table(\n    #--- Header ---\n    header=dict(values=list(df_sample.columns), # Get column names\n                fill_color='paleturquoise',\n                align='left'),\n    \n    #--- Cells ---\n    cells=dict(values=[df_sample[col] for col in df_sample.columns], # Get data\n               fill_color='lavender',\n               align='left'))\n])\n\nfig.update_layout(\n    title=\"Raw Reddit Data (First 200 Rows)\"\n)\n\nfig.show()",
    "crumbs": [
      "View on GitHub",
      "Raw Data"
    ]
  }
]