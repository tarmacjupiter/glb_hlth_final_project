{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fbcd1d7d",
   "metadata": {},
   "source": [
    "Comparing Topic Models for Each Period"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e1dbef5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas\n",
    "import re\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.decomposition import LatentDirichletAllocation\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6ca926c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "%store -r reddit_sent_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a7101f95",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "FileSystemPathPointer('/home/beherya/nltk_data/corpora/stopwords')"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.data.find(\"corpora/stopwords\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8edf04bb",
   "metadata": {},
   "source": [
    "This verifies that the stopwords corpus is installed and returns its location. Interpretation: The stopwords are successfully located, so the code can proceed to use them for text cleaning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e508d64e",
   "metadata": {},
   "outputs": [],
   "source": [
    "base_stopwords = set(stopwords.words(\"english\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ffcd6771",
   "metadata": {},
   "outputs": [],
   "source": [
    "custom_stopwords = set([\n",
    "    'like', 'get', 'dont', 'im', 'would', 'really', 'one', 'people',\n",
    "    'time', 'know', 'feel', 'even', 'go', 'want', 'think', 'much',\n",
    "    'life', 'day', 'days', 'years', 'year', 'something', 'nothing',\n",
    "    'got', 'make', 'feeling', 'going', 'things', 'way', 'work',\n",
    "    'help', 'cant', 'need', 'see', 'friends', 'family', 'ive', 'anyone',\n",
    "    'anything', 'always', 'else', 'getting', 'started'\n",
    "])\n",
    "\n",
    "full_stop_words = base_stopwords.union(custom_stopwords)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f896296",
   "metadata": {},
   "source": [
    "This creates a comprehensive stopword list by combining standard English stopwords with custom ones. Custom stopwords are common words that appear frequently in conversational text but don't carry specific meaning (like \"like\", \"really\", \"things\"). These will be removed from the text to focus on more meaningful content words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "85dfebe4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_text(text):\n",
    "    if not isinstance(text, str):\n",
    "        return \"\"\n",
    "\n",
    "    # Remove URLs\n",
    "    text = re.sub(r'http\\S+|www\\S+', '', text)\n",
    "    # Remove punctuation and numbers\n",
    "    text = re.sub(r'[^a-zA-Z\\s]', '', text)\n",
    "    # Convert to lowercase\n",
    "    text = text.lower()\n",
    "    return text\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f691faf4",
   "metadata": {},
   "source": [
    "This function cleans text by removing URLs, stripping out punctuation and numbers, and converting everything to lowercase. This standardizes the text for analysis.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0a875268",
   "metadata": {},
   "outputs": [],
   "source": [
    "def display_topics(df, text_col, title, n_topics=5, n_top_words=10):\n",
    "    \"\"\"\n",
    "    Runs and prints topic models for a given DataFrame.\n",
    "    \"\"\"\n",
    "    print(\"\\n\" + \"=\"*50)\n",
    "    print(f\" {title} (n={len(df)} posts) \")\n",
    "    print(\"=\"*50)\n",
    "\n",
    "    if len(df) < n_topics:\n",
    "        print(f\"Not enough documents to model {n_topics} topics. Skipping.\")\n",
    "        return\n",
    "\n",
    "    # 1. Vectorize: Convert text to a word-count matrix\n",
    "    # We apply our preprocessing and stopword removal here\n",
    "    vectorizer = CountVectorizer(\n",
    "        preprocessor=preprocess_text,\n",
    "        stop_words=list(full_stop_words),\n",
    "        max_df=0.9,  # Ignore words in > 90% of docs\n",
    "        min_df=10,   # Ignore words in < 10 docs\n",
    "        ngram_range=(1, 1) # Only use single words\n",
    "    )\n",
    "\n",
    "    try:\n",
    "        dtm = vectorizer.fit_transform(df[text_col])\n",
    "    except ValueError as e:\n",
    "        print(f\"Error vectorizing text (maybe all words were stopwords?): {e}\")\n",
    "        return\n",
    "\n",
    "    # 2. Model: Run Latent Dirichlet Allocation\n",
    "    lda = LatentDirichletAllocation(\n",
    "        n_components=n_topics,\n",
    "        random_state=42, # For reproducible results\n",
    "        n_jobs=-1\n",
    "    )\n",
    "    lda.fit(dtm)\n",
    "\n",
    "    # 3. Display: Print the top words for each topic\n",
    "    feature_names = vectorizer.get_feature_names_out()\n",
    "\n",
    "    for topic_idx, topic in enumerate(lda.components_):\n",
    "        # Get the indices of the top words\n",
    "        top_words_indices = topic.argsort()[:-n_top_words - 1:-1]\n",
    "        # Get the words themselves\n",
    "        top_words = [feature_names[i] for i in top_words_indices]\n",
    "        print(f\"Topic {topic_idx + 1}: {' '.join(top_words)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5cc6a352",
   "metadata": {},
   "source": [
    "This is the main function that performs topic modeling using Latent Dirichlet Allocation (LDA). It:\n",
    "\n",
    "Uses CountVectorizer to convert text into a document-term matrix (word counts)\n",
    "Filters out very common words (appearing in >90% of documents) and very rare words (appearing in <10 documents)\n",
    "\n",
    "Fits an LDA model with 5 topics\n",
    "\n",
    "Extracts and displays the top 10 words for each topic\n",
    "\n",
    "The parameters random_state=42 ensures reproducibility, and n_jobs=-1 uses all CPU cores for faster processing.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "93e22ec3",
   "metadata": {},
   "outputs": [],
   "source": [
    "pre_covid_df = reddit_sent_df[reddit_sent_df[\"covid_period\"] == \"Pre-COVID\"]\n",
    "during_covid_df = reddit_sent_df[reddit_sent_df[\"covid_period\"] == \"During COVID\"]\n",
    "post_covid_df = reddit_sent_df[reddit_sent_df[\"covid_period\"] == \"Post-COVID\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a495c62",
   "metadata": {},
   "source": [
    "This splits the Reddit dataset into three time periods to compare how topics changed before, during, and after COVID-19."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5830d9e4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==================================================\n",
      " Pre-COVID Topics (n=576 posts) \n",
      "==================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/beherya/code/schooling/northwestern-senior/glb_hlth_final_project/env/lib/python3.13/site-packages/sklearn/feature_extraction/text.py:402: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['arent', 'couldnt', 'didnt', 'doesnt', 'hadnt', 'hasnt', 'havent', 'hed', 'hell', 'hes', 'id', 'ill', 'isnt', 'itd', 'itll', 'mightnt', 'mustnt', 'neednt', 'shant', 'shed', 'shell', 'shes', 'shouldnt', 'shouldve', 'thatll', 'theyd', 'theyll', 'theyre', 'theyve', 'wasnt', 'wed', 'well', 'werent', 'weve', 'wont', 'wouldnt', 'youd', 'youll', 'youre', 'youve'] not in stop_words.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic 1: fucking everyone alone someone love nye never hate fuck many\n",
      "Topic 2: mental anxiety health didnt first school felt back night since\n",
      "Topic 3: anxiety bad good person happy anxious someone everything tired talk\n",
      "Topic 4: new happy alone everyone better everything anxiety anymore hope shit\n",
      "Topic 5: anxiety panic attack sleep never someone told depression job pain\n",
      "\n",
      "==================================================\n",
      " During-COVID Topics (n=10517 posts) \n",
      "==================================================\n",
      "Topic 1: anxiety anxious panic also sleep attack bad attacks take heart\n",
      "Topic 2: job mental health back home since house didnt last told\n",
      "Topic 3: someone depression thoughts mental talk person love self good thing\n",
      "Topic 4: anymore everything hate never tired every happy fucking better good\n",
      "Topic 5: school didnt never deleted user friend parents mom said told\n",
      "\n",
      "==================================================\n",
      " Post-COVID Topics (n=6218 posts) \n",
      "==================================================\n",
      "Topic 1: end talk didnt someone say said friend never thing told\n",
      "Topic 2: job never mental live love mom parents health school hate\n",
      "Topic 3: depression better anymore everything thoughts good bad happy tired never\n",
      "Topic 4: anxiety heart symptoms sleep also pain panic back scared health\n",
      "Topic 5: anxiety panic home also anxious take night attacks back attack\n"
     ]
    }
   ],
   "source": [
    "display_topics(pre_covid_df, 'full_text', title=\"Pre-COVID Topics\")\n",
    "display_topics(during_covid_df, 'full_text', title=\"During-COVID Topics\")\n",
    "display_topics(post_covid_df, 'full_text', title=\"Post-COVID Topics\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "340fa3c1",
   "metadata": {},
   "source": [
    "## Pre-COVID Topics"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4f325d6",
   "metadata": {},
   "source": [
    "(n=576 posts)\n",
    "Interpretation: With only 576 posts, this is your smallest dataset. The topics show:\n",
    "\n",
    "Topic 1: Social isolation and loneliness (\"alone\", \"everyone\", \"nye\" suggesting New Year's Eve loneliness)\n",
    "\n",
    "Topic 2: Mental health struggles, particularly anxiety related to school (\"mental\", \"anxiety\", \"health\", \"school\")\n",
    "\n",
    "Topic 3: General anxiety and emotional states (\"anxiety\", \"anxious\", \"happy\", \"tired\")\n",
    "\n",
    "Topic 4: Hope and improvement (\"new\", \"happy\", \"better\", \"hope\") mixed with anxiety\n",
    "\n",
    "Topic 5: Clinical anxiety symptoms (\"panic\", \"attack\", \"sleep\", \"depression\")\n",
    "\n",
    "\n",
    "Key insight: Even before COVID, anxiety and mental health were prominent topics, with distinct themes around panic attacks and social isolation.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7721736",
   "metadata": {},
   "source": [
    "## During COVID Topics"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c42c861",
   "metadata": {},
   "source": [
    "(n=10,517 posts)\n",
    "The topics show:\n",
    "\n",
    "Topic 1: Physical anxiety symptoms (\"panic\", \"attack\", \"sleep\", \"heart\") - very clinical\n",
    "\n",
    "Topic 2: Life disruption (\"job\", \"mental health\", \"home\", \"house\") - likely reflecting lockdown impacts\n",
    "\n",
    "Topic 3: Deep emotional struggles (\"depression\", \"thoughts\", \"mental\", \"self\") - more serious mental \n",
    "health concerns\n",
    "\n",
    "Topic 4: Exhaustion and negativity (\"anymore\", \"hate\", \"tired\", \"never\", \"fucking\")\n",
    "\n",
    "Topic 5: Youth and family dynamics (\"school\", \"friend\", \"parents\", \"mom\") - note \"deleted user\" suggests some users deleted their accounts\n",
    "\n",
    "\n",
    "Key insight: The dramatic increase in posts and the shift toward more serious mental health language (depression, suicidal ideation implied by \"thoughts\") suggests COVID significantly intensified mental health struggles, particularly around isolation, disrupted routines, and family stress."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc097428",
   "metadata": {},
   "source": [
    "## Post-COVID Topics"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00469526",
   "metadata": {},
   "source": [
    "About 60% of the during-COVID volume, but still 10x higher than pre-COVID. Topics show:\n",
    "\n",
    "Topic 1: Communication and relationships (\"talk\", \"friend\", \"say\", \"told\")\n",
    "\n",
    "Topic 2: Life circumstances and family (\"job\", \"live\", \"parents\", \"school\")\n",
    "\n",
    "Topic 3: Depression and emotional fatigue (\"depression\", \"tired\", \"thoughts\")\n",
    "\n",
    "Topic 4: Physical anxiety symptoms persist (\"anxiety\", \"heart\", \"symptoms\", \"pain\", \"panic\")\n",
    "\n",
    "Topic 5: Continued panic disorder (\"panic\", \"anxious\", \"attacks\")\n",
    "\n",
    "\n",
    "Key insight: While volume decreased from during-COVID, it remains much higher than pre-COVID levels, suggesting lasting mental health impacts. The persistence of clinical anxiety symptoms (Topics 4 & 5) indicates that anxiety disorders triggered or worsened during COVID haven't fully resolved.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
